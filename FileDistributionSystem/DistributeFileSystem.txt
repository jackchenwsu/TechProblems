
╭─── Claude Code v2.0.76 ──────────────────────────────────────────────────────╮
│                                                    │ Tips for getting        │
│                 Welcome back Chao!                 │ started                 │
│                                                    │ Ask Claude to create a… │
│                    * ▗ ▗   ▖ ▖ *                   │ ─────────────────────── │
│                   *             *                  │ Recent activity         │
│                    *   ▘▘ ▝▝   *                   │ No recent activity      │
│                                                    │                         │
│    Opus 4.5 · API Usage Billing · Experiences +    │                         │
│    Devices, Microsoft Corporation                  │                         │
│       ~/Documents/git/FileDistributionSystem       │                         │
╰──────────────────────────────────────────────────────────────────────────────╯

> pwd 

⏺ Bash(pwd)
  ⎿  /Users/chaochen/Documents/git

⏺ /Users/chaochen/Documents/git

> mkdir FileDistributionSystem 

⏺ Bash(mkdir FileDistributionSystem)
  ⎿  (No content)

⏺ Done. Created FileDistributionSystem directory.

> go to the folder FileDistributionSystem 

⏺ Bash(cd FileDistributionSystem && pwd)
  ⎿  /Users/chaochen/Documents/git/FileDistributionSystem

⏺ Now in /Users/chaochen/Documents/git/FileDistributionSystem

> I am working on a design problem. Could you help me to understand the 
problem and list all potential requirements? 'Design a distributed file 
system (a file system that works across multiple computers). It needs to 
handle these common actions:

Create a directory (folder)
List what is inside a directory
Put a file (upload)
Get a file (download)
Delete files and directories
What You Need to Build

System Capabilities:

Perform standard actions: create, read, update, and delete.
Handle folders inside other folders (recursive hierarchy).
Allow file uploads and downloads.
Allow users to see a list of files in a directory.
Performance & Quality Goals:

Strong consistency: This is the most important part. Unlike S3, when you add
 or change a file, the "list" command must show the change immediately. 
"Last-writer-wins" is not allowed.
Scalability: The system must handle very large files efficiently.
Availability: The system must stay online even if some servers fail.' 

⏺ Distributed File System - Requirements Analysis

  Core Functional Requirements (Explicit)

  File Operations

  - Create directory - Make new folders at any path
  - List directory - Show contents of a folder (files + subdirectories)
  - Put file - Upload files to the system
  - Get file - Download files from the system
  - Delete - Remove files and directories

  Hierarchy Support

  - Nested directories (folders within folders)
  - Recursive operations (e.g., delete a folder with contents)

  ---
  Non-Functional Requirements (Explicit)

  | Requirement        | Description                                                                                 |
  |--------------------|---------------------------------------------------------------------------------------------|
  | Strong Consistency | Read-after-write guarantee. List must immediately reflect changes. No eventual consistency. |
  | Large File Support | Efficient handling of big files (chunking, streaming)                                       |
  | High Availability  | System remains operational during partial failures                                          |

  ---
  Implicit/Additional Requirements to Consider

  1. Metadata Management

  - File size, timestamps (created, modified)
  - File type/extension
  - Owner/permissions
  - Path resolution

  2. Concurrency Control

  - How to handle simultaneous writes to same file?
  - Directory-level locking vs file-level locking?
  - Conflict resolution strategy (since "last-writer-wins" is disallowed)

  3. Data Integrity

  - Checksums for file corruption detection
  - Atomic operations (partial writes shouldn't leave corrupted state)

  4. Scalability Dimensions

  - Number of files/directories
  - Number of concurrent users
  - Geographic distribution?
  - Storage capacity growth

  5. Fault Tolerance

  - Replication factor (how many copies of data?)
  - Recovery from node failures
  - Data durability guarantees

  6. Security

  - Authentication (who can access?)
  - Authorization (read/write permissions per user?)
  - Encryption (at rest? in transit?)

  7. API Design

  - REST? gRPC? Custom protocol?
  - Client SDK requirements

  ---
  Key Design Tensions

  | Trade-off                   | Consideration                                                                      |
  |-----------------------------|------------------------------------------------------------------------------------|
  | Consistency vs Availability | Strong consistency requirement limits availability during partitions (CAP theorem) |
  | Consistency vs Performance  | Synchronous replication adds latency                                               |
  | Large files vs Consistency  | Chunked uploads complicate atomic visibility                                       |

  ---
  Clarifying Questions

  1. Multi-user access? Should multiple users share the same namespace or have isolated spaces?
  2. Permissions model? Read-only vs read-write per user/file?
  3. File versioning? Keep history or just latest version?
  4. Rename/move operations? Atomic move across directories?
  5. Max file size? GB? TB?
  6. Expected scale? Number of files, total storage, QPS?
  7. Deployment model? Single datacenter or multi-region?

  Would you like to dive deeper into any specific requirement area, or shall we start designing the system architecture?

> Let us start design the system architecture 

⏺ Distributed File System Architecture

  High-Level Architecture Overview

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                              CLIENTS                                         │
  │                    (SDK / CLI / Web Interface)                               │
  └─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                           API GATEWAY / LOAD BALANCER                        │
  └─────────────────────────────────────────────────────────────────────────────┘
                                      │
                   ┌──────────────────┴──────────────────┐
                   ▼                                     ▼
  ┌────────────────────────────────┐    ┌────────────────────────────────────────┐
  │      METADATA SERVICE          │    │         STORAGE SERVICE                │
  │      (Control Plane)           │    │         (Data Plane)                   │
  │                                │    │                                        │
  │  ┌──────────────────────────┐  │    │  ┌────────────────────────────────┐   │
  │  │   Metadata Leader        │  │    │  │     Chunk Server 1             │   │
  │  │   (Raft/Paxos)           │  │    │  │     ┌─────┬─────┬─────┐       │   │
  │  └──────────────────────────┘  │    │  │     │ C1  │ C4  │ C7  │       │   │
  │             │                  │    │  │     └─────┴─────┴─────┘       │   │
  │             ▼                  │    │  └────────────────────────────────┘   │
  │  ┌──────────────────────────┐  │    │  ┌────────────────────────────────┐   │
  │  │   Metadata Followers     │  │    │  │     Chunk Server 2             │   │
  │  │   (Replicas)             │  │    │  │     ┌─────┬─────┬─────┐       │   │
  │  └──────────────────────────┘  │    │  │     │ C2  │ C5  │ C1' │       │   │
  │             │                  │    │  │     └─────┴─────┴─────┘       │   │
  │             ▼                  │    │  └────────────────────────────────┘   │
  │  ┌──────────────────────────┐  │    │  ┌────────────────────────────────┐   │
  │  │   Metadata Store         │  │    │  │     Chunk Server 3             │   │
  │  │   (Distributed DB)       │  │    │  │     ┌─────┬─────┬─────┐       │   │
  │  └──────────────────────────┘  │    │  │     │ C3  │ C6  │ C4' │       │   │
  └────────────────────────────────┘    │  │     └─────┴─────┴─────┘       │   │
                                        │  └────────────────────────────────┘   │
                                        └────────────────────────────────────────┘

  ---
  Core Components

  1. Client

  - SDK/CLI for applications to interact with the file system
  - Handles chunking of large files before upload
  - Caches metadata locally (with invalidation)

  2. API Gateway

  - Single entry point for all requests
  - Authentication & rate limiting
  - Routes requests to appropriate services

  3. Metadata Service (Control Plane)

  The brain of the system - manages all file/directory information.

  | Responsibility           | Details                           |
  |--------------------------|-----------------------------------|
  | Namespace management     | Directory tree structure          |
  | File → Chunk mapping     | Which chunks belong to which file |
  | Chunk → Server mapping   | Where each chunk is stored        |
  | Consistency coordination | Ensures strong consistency        |
  | Locking                  | Prevents conflicting writes       |

  Consistency Approach: Uses Raft consensus for leader election and replicated state machine.

  4. Storage Service (Data Plane)

  Stores actual file data as chunks.

  | Responsibility     | Details                           |
  |--------------------|-----------------------------------|
  | Chunk storage      | Store/retrieve file chunks        |
  | Replication        | Maintain N copies of each chunk   |
  | Health reporting   | Report status to Metadata Service |
  | Garbage collection | Clean up orphaned chunks          |

  ---

⏺ Data Model

  Metadata Schema

  ┌─────────────────────────────────────────────────────────────────┐
  │                         INODE TABLE                             │
  ├─────────────────────────────────────────────────────────────────┤
  │  inode_id     │ BIGINT PRIMARY KEY (globally unique)           │
  │  parent_id    │ BIGINT (reference to parent directory)         │
  │  name         │ VARCHAR (file/directory name)                  │
  │  type         │ ENUM (FILE, DIRECTORY)                         │
  │  size         │ BIGINT (bytes, 0 for directories)              │
  │  created_at   │ TIMESTAMP                                      │
  │  modified_at  │ TIMESTAMP                                      │
  │  owner        │ VARCHAR                                        │
  │  permissions  │ INT (Unix-style permissions)                   │
  │  version      │ BIGINT (for optimistic locking)                │
  └─────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────┐
  │                         CHUNK TABLE                             │
  ├─────────────────────────────────────────────────────────────────┤
  │  chunk_id     │ UUID PRIMARY KEY                               │
  │  inode_id     │ BIGINT (FK to INODE)                           │
  │  chunk_index  │ INT (0, 1, 2... order in file)                 │
  │  size         │ INT (bytes in this chunk)                      │
  │  checksum     │ VARCHAR (SHA-256)                              │
  │  servers      │ ARRAY<server_id> (where replicas live)         │
  └─────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────┐
  │                      CHUNK SERVER TABLE                         │
  ├─────────────────────────────────────────────────────────────────┤
  │  server_id    │ UUID PRIMARY KEY                               │
  │  address      │ VARCHAR (host:port)                            │
  │  capacity     │ BIGINT (total bytes)                           │
  │  used         │ BIGINT (used bytes)                            │
  │  status       │ ENUM (ONLINE, OFFLINE, DRAINING)               │
  │  last_heartbeat│ TIMESTAMP                                     │
  └─────────────────────────────────────────────────────────────────┘

  File Chunking Strategy

  Large File (e.g., 256 MB)
  ┌──────────────────────────────────────────────────────────────────┐
  │                                                                  │
  └──────────────────────────────────────────────────────────────────┘
                                │
                                ▼  Split into fixed-size chunks (64 MB default)
          ┌─────────────┬─────────────┬─────────────┬─────────────┐
          │  Chunk 0    │  Chunk 1    │  Chunk 2    │  Chunk 3    │
          │   64 MB     │   64 MB     │   64 MB     │   64 MB     │
          └─────────────┴─────────────┴─────────────┴─────────────┘
                │             │             │             │
                ▼             ▼             ▼             ▼
           Replicated    Replicated    Replicated    Replicated
           to 3 servers  to 3 servers  to 3 servers  to 3 servers

  Chunk size: 64 MB (configurable)
  - Large enough to reduce metadata overhead
  - Small enough for parallel transfers and efficient replication

  ---

⏺ Consistency Mechanism (Critical Requirement)

  Since strong consistency is the top priority, we use a two-phase approach:

  Metadata Consistency: Raft Consensus

  ┌─────────────────────────────────────────────────────────────────┐
  │                    METADATA RAFT CLUSTER                        │
  │                                                                 │
  │    ┌─────────┐       ┌─────────┐       ┌─────────┐             │
  │    │ Leader  │◄─────►│Follower │◄─────►│Follower │             │
  │    │  Node   │       │  Node   │       │  Node   │             │
  │    └─────────┘       └─────────┘       └─────────┘             │
  │         │                                                       │
  │         ▼                                                       │
  │    All writes go through leader                                 │
  │    Reads can go to leader (linearizable) or followers (stale)  │
  └─────────────────────────────────────────────────────────────────┘

  - All metadata writes go through Raft leader
  - Write is acknowledged only after majority replication
  - Guarantees: linearizable reads and writes

  File Write Consistency: Write-Ahead + Atomic Commit

  Client                Metadata Service           Chunk Servers
    │                         │                         │
    │  1. Create file         │                         │
    │  ───────────────────►   │                         │
    │                         │                         │
    │  2. Return chunk IDs    │                         │
    │     + server locations  │                         │
    │  ◄───────────────────   │                         │
    │                         │                         │
    │  3. Upload chunks (parallel)                      │
    │  ─────────────────────────────────────────────►   │
    │                         │                         │
    │  4. All chunks ACK      │                         │
    │  ◄─────────────────────────────────────────────   │
    │                         │                         │
    │  5. COMMIT file         │                         │
    │  ───────────────────►   │                         │
    │                         │                         │
    │  6. Metadata updated    │                         │
    │     (visible in LIST)   │                         │
    │  ◄───────────────────   │                         │
    │                         │                         │

  Key Principle: File is NOT visible until COMMIT succeeds

  Handling Concurrent Writes (No Last-Writer-Wins)

  Approach: Pessimistic Locking with Lease

  ┌─────────────────────────────────────────────────────────────────┐
  │                      WRITE LOCK FLOW                            │
  ├─────────────────────────────────────────────────────────────────┤
  │  1. Client A requests lock on /path/file.txt                    │
  │  2. Metadata Service grants lock with lease (e.g., 60 seconds)  │
  │  3. Client B tries to write → BLOCKED (gets error or waits)    │
  │  4. Client A completes write → releases lock                   │
  │  5. Client B can now acquire lock                               │
  └─────────────────────────────────────────────────────────────────┘

  Alternative: Optimistic Locking with Version
  1. Read file with version V1
  2. Modify locally
  3. Write with "expected_version = V1"
  4. If current version != V1 → CONFLICT error (client must retry)

  ---

⏺ File Operations Flow

  CREATE DIRECTORY

  Client                    Metadata Service
    │                             │
    │  mkdir("/a/b/newdir")       │
    │  ───────────────────────►   │
    │                             │  1. Validate parent "/a/b" exists
    │                             │  2. Check permissions
    │                             │  3. Check "newdir" doesn't exist
    │                             │  4. Acquire lock on parent
    │                             │  5. Create inode via Raft
    │                             │  6. Release lock
    │  Success                    │
    │  ◄───────────────────────   │

  LIST DIRECTORY

  Client                    Metadata Service
    │                             │
    │  ls("/a/b")                 │
    │  ───────────────────────►   │
    │                             │  1. Read from Raft leader
    │                             │     (ensures latest data)
    │  [file1, file2, dir1]       │
    │  ◄───────────────────────   │

  PUT FILE (Upload)

  Client                 Metadata Service              Chunk Servers
    │                          │                             │
    │ 1. PUT /a/file.txt       │                             │
    │    (size=150MB)          │                             │
    │ ──────────────────────►  │                             │
    │                          │ 2. Allocate 3 chunk IDs     │
    │                          │    Select servers           │
    │ 3. chunk_plan returned   │                             │
    │ ◄──────────────────────  │                             │
    │                          │                             │
    │ 4. Upload chunk 0 ───────────────────────────────────► │ Server A,B,C
    │ 5. Upload chunk 1 ───────────────────────────────────► │ Server D,E,F
    │ 6. Upload chunk 2 ───────────────────────────────────► │ Server G,H,I
    │                          │                             │
    │ 7. All ACKs received     │                             │
    │ ◄────────────────────────────────────────────────────  │
    │                          │                             │
    │ 8. COMMIT request        │                             │
    │ ──────────────────────►  │                             │
    │                          │ 9. Mark file VISIBLE        │
    │                          │    (Raft commit)            │
    │ 10. Success              │                             │
    │ ◄──────────────────────  │                             │

  GET FILE (Download)

  Client                 Metadata Service              Chunk Servers
    │                          │                             │
    │ 1. GET /a/file.txt       │                             │
    │ ──────────────────────►  │                             │
    │                          │                             │
    │ 2. Return chunk list     │                             │
    │    + server locations    │                             │
    │ ◄──────────────────────  │                             │
    │                          │                             │
    │ 3. Download chunk 0 ─────────────────────────────────► │ (parallel)
    │ 4. Download chunk 1 ─────────────────────────────────► │
    │ 5. Download chunk 2 ─────────────────────────────────► │
    │                          │                             │
    │ 6. Reassemble file       │                             │
    │    (verify checksums)    │                             │

  DELETE

  Client                 Metadata Service              Chunk Servers
    │                          │                             │
    │ 1. DELETE /a/file.txt    │                             │
    │ ──────────────────────►  │                             │
    │                          │ 2. Acquire lock             │
    │                          │ 3. Mark as DELETED (Raft)   │
    │                          │ 4. Release lock             │
    │ Success                  │                             │
    │ ◄──────────────────────  │                             │
    │                          │                             │
    │                          │ 5. Async: Add chunks to     │
    │                          │    garbage collection queue │
    │                          │                             │
    │                          │ 6. Background GC ──────────►│ Delete chunks

  ---

⏺ Scalability Strategy

  Metadata Scalability

  Challenge: Single Raft cluster limits metadata throughput

  Solution: Namespace Partitioning (Sharding)

  ┌─────────────────────────────────────────────────────────────────┐
  │                     NAMESPACE PARTITIONING                      │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   /users/a-m/**  ──────►  Metadata Shard 1 (Raft cluster)      │
  │   /users/n-z/**  ──────►  Metadata Shard 2 (Raft cluster)      │
  │   /data/**       ──────►  Metadata Shard 3 (Raft cluster)      │
  │                                                                 │
  │   Router layer maps path prefix → shard                        │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Cross-shard operations (e.g., move file between shards): Use 2-phase commit

  Storage Scalability

  ┌─────────────────────────────────────────────────────────────────┐
  │                   HORIZONTAL STORAGE SCALING                    │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Need more storage? Add more chunk servers                     │
  │                                                                 │
  │   ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐       │
  │   │Server 1│ │Server 2│ │Server 3│ │Server 4│ │Server 5│  ...  │
  │   │ 10 TB  │ │ 10 TB  │ │ 10 TB  │ │ 10 TB  │ │ 10 TB  │       │
  │   └────────┘ └────────┘ └────────┘ └────────┘ └────────┘       │
  │                                                                 │
  │   - New servers register with Metadata Service                  │
  │   - Receive new chunk allocations automatically                 │
  │   - Background rebalancing redistributes existing data          │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Large File Handling

  | Technique             | Benefit                                           |
  |-----------------------|---------------------------------------------------|
  | Parallel chunk upload | Utilize multiple servers' bandwidth               |
  | Streaming             | Don't load entire file in memory                  |
  | Resumable uploads     | Client tracks uploaded chunks, resumes on failure |
  | Compression           | Optional per-chunk compression                    |

  ---

⏺ Availability & Fault Tolerance

  Metadata High Availability

  ┌─────────────────────────────────────────────────────────────────┐
  │                    RAFT FAILURE HANDLING                        │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Normal: 5 nodes, Leader + 4 Followers                         │
  │                                                                 │
  │   ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐                 │
  │   │  L  │  │  F  │  │  F  │  │  F  │  │  F  │                 │
  │   └─────┘  └─────┘  └─────┘  └─────┘  └─────┘                 │
  │                                                                 │
  │   Leader fails:                                                 │
  │   ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐                 │
  │   │  X  │  │ L*  │  │  F  │  │  F  │  │  F  │                 │
  │   └─────┘  └─────┘  └─────┘  └─────┘  └─────┘                 │
  │                     ▲                                           │
  │                     └── New leader elected (~seconds)           │
  │                                                                 │
  │   Tolerate: 2 node failures (majority = 3 still available)     │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Chunk Replication & Recovery

  ┌─────────────────────────────────────────────────────────────────┐
  │                    CHUNK REPLICATION                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Replication Factor: 3 (configurable)                          │
  │                                                                 │
  │   Chunk C1 replicated to:                                       │
  │   ┌──────────┐  ┌──────────┐  ┌──────────┐                     │
  │   │ Server A │  │ Server B │  │ Server C │                     │
  │   │   (C1)   │  │   (C1')  │  │   (C1'') │                     │
  │   │  PRIMARY │  │ SECONDARY│  │ SECONDARY│                     │
  │   └──────────┘  └──────────┘  └──────────┘                     │
  │                                                                 │
  │   Placement strategy:                                           │
  │   - Different racks (if available)                              │
  │   - Balance storage utilization                                 │
  │   - Minimize network distance for reads                         │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Server Failure Recovery

  ┌─────────────────────────────────────────────────────────────────┐
  │                   CHUNK SERVER FAILURE                          │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  1. Heartbeat timeout (e.g., 30 seconds)                        │
  │     Metadata Service marks server OFFLINE                       │
  │                                                                 │
  │  2. Identify under-replicated chunks                            │
  │     (chunks that now have < 3 replicas)                         │
  │                                                                 │
  │  3. Schedule re-replication                                     │
  │     Copy from surviving replica to new server                   │
  │                                                                 │
  │  Timeline:                                                      │
  │  ──────────────────────────────────────────────────────────►    │
  │  │ Failure │ Detection │ Re-replication │ Restored │            │
  │  │   t=0   │  t=30s    │    t=30s+      │  t=?     │            │
  │                                                                 │
  │  Reads continue from surviving replicas (no downtime)           │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  CAP Trade-off Decision

  Given strong consistency requirement:

  ┌─────────────────────────────────────────────────────────────────┐
  │                      CAP TRADE-OFF                              │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   We choose: CP (Consistency + Partition tolerance)             │
  │                                                                 │
  │   During network partition:                                     │
  │   - Minority partition: UNAVAILABLE (cannot serve writes)      │
  │   - Majority partition: AVAILABLE (continues serving)          │
  │                                                                 │
  │   This matches the requirement that "list must show changes     │
  │   immediately" - we never serve stale data                     │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---

⏺ Summary: Key Design Decisions

  | Aspect               | Decision                                        | Rationale                                          |
  |----------------------|-------------------------------------------------|----------------------------------------------------|
  | Separation           | Control plane (metadata) vs Data plane (chunks) | Independent scaling, different consistency needs   |
  | Metadata consistency | Raft consensus                                  | Strong consistency guarantee                       |
  | Chunk size           | 64 MB                                           | Balance between overhead and parallelism           |
  | Replication          | 3 replicas per chunk                            | Survive 2 failures                                 |
  | Write visibility     | Atomic commit after all chunks uploaded         | Ensures LIST always shows complete files           |
  | Concurrent writes    | Pessimistic locking with lease                  | Prevents last-writer-wins                          |
  | CAP choice           | CP                                              | Strong consistency > availability during partition |

  ---
  Technology Choices (If Implementing)

  | Component          | Options                        |
  |--------------------|--------------------------------|
  | Metadata consensus | etcd, Raft (custom), ZooKeeper |
  | Metadata storage   | RocksDB, LevelDB, PostgreSQL   |
  | Chunk storage      | Local filesystem, object store |
  | RPC framework      | gRPC, Thrift                   |
  | Client             | Go, Python, Java SDK           |

  ---
  Would you like me to:
  1. Dive deeper into any specific component?
  2. Create API specifications?
  3. Start implementing a prototype?
  4. Discuss edge cases and failure scenarios?

> Which database will you pick and why? 

⏺ Database Selection for Distributed File System

  Two Storage Needs

  | Layer            | Purpose                                    | Access Pattern                       |
  |------------------|--------------------------------------------|--------------------------------------|
  | Coordination     | Leader election, distributed locks, config | Small data, high consistency         |
  | Metadata Storage | Inodes, chunks, server registry            | Large data, range queries, hierarchy |

  ---
  My Recommendation

  Option A: etcd + Embedded RocksDB (Recommended for Most Cases)

  ┌─────────────────────────────────────────────────────────────────┐
  │                      RECOMMENDED STACK                          │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   ┌─────────────────────────────────────────┐                  │
  │   │              etcd Cluster                │                  │
  │   │  - Leader election                       │                  │
  │   │  - Distributed locks                     │                  │
  │   │  - Small critical metadata               │                  │
  │   └─────────────────────────────────────────┘                  │
  │                         │                                       │
  │                         ▼                                       │
  │   ┌─────────────────────────────────────────┐                  │
  │   │    Metadata Service (Raft replicated)   │                  │
  │   │    ┌─────────────────────────────────┐  │                  │
  │   │    │         RocksDB (embedded)      │  │                  │
  │   │    │  - Inode table                  │  │                  │
  │   │    │  - Chunk mappings               │  │                  │
  │   │    │  - Directory listings           │  │                  │
  │   │    └─────────────────────────────────┘  │                  │
  │   └─────────────────────────────────────────┘                  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Why etcd:
  | Benefit         | Details                             |
  |-----------------|-------------------------------------|
  | Built on Raft   | Strong consistency out of the box   |
  | Battle-tested   | Powers Kubernetes at massive scale  |
  | Watch support   | Real-time notifications for changes |
  | Lease mechanism | Natural fit for distributed locks   |

  Why RocksDB:
  | Benefit          | Details                                |
  |------------------|----------------------------------------|
  | LSM-tree         | Excellent write throughput             |
  | Embedded         | No network hop, low latency            |
  | Prefix iteration | Efficient directory listing (/a/b/*)   |
  | Snapshots        | Consistent reads during compaction     |
  | Proven           | Used by CockroachDB, TiKV, many others |

  ---
  Option B: TiKV (For Larger Scale)

  ┌─────────────────────────────────────────────────────────────────┐
  │                     TIKV ARCHITECTURE                           │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   ┌──────────┐  ┌──────────┐  ┌──────────┐                     │
  │   │  TiKV    │  │  TiKV    │  │  TiKV    │                     │
  │   │  Node 1  │  │  Node 2  │  │  Node 3  │  ...                │
  │   │ (Region) │  │ (Region) │  │ (Region) │                     │
  │   └──────────┘  └──────────┘  └──────────┘                     │
  │        │              │              │                          │
  │        └──────────────┼──────────────┘                          │
  │                       ▼                                         │
  │              ┌─────────────────┐                                │
  │              │   PD Cluster    │  (Placement Driver)            │
  │              │   - Scheduling  │                                │
  │              │   - Raft groups │                                │
  │              └─────────────────┘                                │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Why TiKV:
  | Benefit       | Details                                            |
  |---------------|----------------------------------------------------|
  | Auto-sharding | Scales horizontally without manual intervention    |
  | Multi-Raft    | Each region has own Raft group, better parallelism |
  | Transactions  | Distributed ACID transactions                      |
  | MVCC          | Snapshot isolation for reads                       |

  Trade-off: More operational complexity

  ---
  Option C: FoundationDB (If Apple-Scale)

  Best for: Massive scale, need flexible data models
  Used by: Apple (iCloud), Snowflake
  Pros: Incredible consistency guarantees, layered architecture
  Cons: Steeper learning curve, smaller community

  ---
  Why NOT These Options

  | Database   | Reason to Avoid                                                               |
  |------------|-------------------------------------------------------------------------------|
  | PostgreSQL | Single-node bottleneck; CockroachDB/Spanner alternatives add latency          |
  | MongoDB    | Historically weaker consistency; not ideal for strong consistency requirement |
  | Cassandra  | Eventual consistency by design; AP system, not CP                             |
  | Redis      | In-memory limits scale; not designed for primary storage                      |
  | ZooKeeper  | Older, more complex than etcd; limited data size                              |
  | MySQL      | Same scaling issues as PostgreSQL                                             |

  ---
  Data Model in RocksDB

  Using prefix-based keys for efficient operations:

  ┌─────────────────────────────────────────────────────────────────┐
  │                     KEY DESIGN                                  │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  INODE by ID:                                                   │
  │  Key:   i:{inode_id}                                           │
  │  Value: {name, type, size, parent_id, timestamps, ...}         │
  │                                                                 │
  │  DIRECTORY CHILDREN (for listing):                              │
  │  Key:   d:{parent_inode_id}:{child_name}                       │
  │  Value: {child_inode_id}                                       │
  │                                                                 │
  │  CHUNKS by FILE:                                                │
  │  Key:   c:{inode_id}:{chunk_index}                             │
  │  Value: {chunk_id, checksum, servers[]}                        │
  │                                                                 │
  │  Example - List /home/user:                                     │
  │  PrefixScan("d:12345:") → all children of inode 12345          │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Final Recommendation

  | Scale           | Recommendation                                 |
  |-----------------|------------------------------------------------|
  | < 100M files    | etcd + RocksDB (simpler ops, proven)           |
  | 100M - 1B files | TiKV (auto-sharding, distributed transactions) |
  | > 1B files      | FoundationDB or custom sharded solution        |

  For a typical enterprise distributed file system, etcd + RocksDB provides the best balance of:
  - Strong consistency (Raft)
  - Performance (embedded, LSM-tree)
  - Operational simplicity
  - Proven reliability

  Would you like me to detail the RocksDB schema design or discuss how to implement the Raft replication layer?

> How do you delete a directory that contains many other directories inside 
it? 

⏺ Recursive Directory Deletion

  This is a challenging problem because deleting /data might involve millions of files and subdirectories. A naive approach could take hours and leave the system in an inconsistent state if it fails midway.

  The Challenge

  DELETE /data
      │
      ├── /data/projects/           (1000 subdirs)
      │   ├── /data/projects/a/     (10000 files)
      │   ├── /data/projects/b/     (10000 files)
      │   └── ...
      ├── /data/backups/            (500 subdirs)
      │   └── ...
      └── /data/logs/               (1M files)
          └── ...

  Total: 10M+ inodes to delete

  ---
  Approach 1: Synchronous Recursive Delete (Simple but Dangerous)

  function deleteRecursive(path):
      for child in listDirectory(path):
          if child.isDirectory():
              deleteRecursive(child.path)    # Recurse
          else:
              deleteFile(child.path)         # Delete file
      deleteEmptyDirectory(path)             # Delete self

  Problems:
  | Issue                   | Impact                     |
  |-------------------------|----------------------------|
  | Long blocking operation | Client timeout             |
  | Partial failure         | Orphaned subtrees          |
  | No atomicity            | Inconsistent state visible |
  | Holds locks too long    | Blocks other operations    |

  ---
  Approach 2: Lazy Deletion with Tombstone (Recommended)

  Key Idea: Mark the directory as deleted instantly, clean up asynchronously.

  ┌─────────────────────────────────────────────────────────────────┐
  │                    LAZY DELETION FLOW                           │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Phase 1: INSTANT (User-facing)                                 │
  │  ─────────────────────────────────                              │
  │  1. Mark directory with DELETED tombstone                       │
  │  2. Remove from parent's children list                          │
  │  3. Return SUCCESS to client                                    │
  │                                                                 │
  │  User sees: Immediate deletion ✓                                │
  │                                                                 │
  │  Phase 2: ASYNC (Background)                                    │
  │  ─────────────────────────────                                  │
  │  4. Queue directory for garbage collection                      │
  │  5. GC worker traverses and deletes children                    │
  │  6. Chunks added to chunk GC queue                              │
  │  7. Chunk servers delete actual data                            │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Implementation Detail

  ┌─────────────────────────────────────────────────────────────────┐
  │                     METADATA CHANGES                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  INODE TABLE - Add status field:                                │
  │  ┌──────────────────────────────────────────────────────────┐  │
  │  │ inode_id │ name │ status     │ deleted_at │ ...         │  │
  │  ├──────────────────────────────────────────────────────────┤  │
  │  │ 12345    │ data │ DELETED    │ 2024-01-15 │ ...         │  │
  │  │ 12346    │ foo  │ ACTIVE     │ NULL       │ ...         │  │
  │  └──────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │  DELETION QUEUE TABLE:                                          │
  │  ┌──────────────────────────────────────────────────────────┐  │
  │  │ queue_id │ inode_id │ status     │ created_at │ retries │  │
  │  ├──────────────────────────────────────────────────────────┤  │
  │  │ 1        │ 12345    │ PENDING    │ 2024-01-15 │ 0       │  │
  │  └──────────────────────────────────────────────────────────┘  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Garbage Collection Worker

  while true:
      # Pick a directory from deletion queue
      dir = dequeue(DELETION_QUEUE, status=PENDING)
      if dir is None:
          sleep(1 second)
          continue

      markInProgress(dir)

      # Process in batches to avoid overwhelming system
      batch_size = 1000
      children = getChildren(dir.inode_id, limit=batch_size)

      for child in children:
          if child.isDirectory():
              # Queue subdirectory for deletion (breadth-first)
              enqueue(DELETION_QUEUE, child.inode_id)
          else:
              # Queue file's chunks for deletion
              chunks = getChunks(child.inode_id)
              enqueue(CHUNK_GC_QUEUE, chunks)
              deleteInode(child.inode_id)

      if no more children:
          deleteInode(dir.inode_id)
          markCompleted(dir)
      else:
          # More children remain, re-queue
          requeue(dir)

  ---
  Handling Edge Cases

  1. What if someone tries to access /data/projects/a/file.txt during deletion?

  ┌─────────────────────────────────────────────────────────────────┐
  │                    PATH RESOLUTION                              │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  GET /data/projects/a/file.txt                                  │
  │                                                                 │
  │  1. Resolve /data         → inode 12345 (status: DELETED)      │
  │  2. Return ERROR: "No such file or directory"                   │
  │                                                                 │
  │  Rule: If ANY ancestor is DELETED, path doesn't exist          │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  2. What if GC worker crashes mid-deletion?

  ┌─────────────────────────────────────────────────────────────────┐
  │                    CRASH RECOVERY                               │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Deletion queue entries have:                                   │
  │  - status: PENDING → IN_PROGRESS → COMPLETED                   │
  │  - last_heartbeat: timestamp                                    │
  │  - retries: count                                               │
  │                                                                 │
  │  Recovery:                                                      │
  │  - Entries stuck IN_PROGRESS > 5 minutes → reset to PENDING    │
  │  - Another worker picks up and continues                        │
  │  - Idempotent operations: safe to re-delete already-deleted    │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  3. What if user creates /data again before GC completes?

  ┌─────────────────────────────────────────────────────────────────┐
  │                    NAME REUSE                                   │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Option A: Allow immediately (new inode_id)                     │
  │  - Old inode 12345 still being GC'd                             │
  │  - New inode 99999 created for /data                            │
  │  - No conflict (different inode_ids)                            │
  │                                                                 │
  │  Option B: Block until GC complete                              │
  │  - Safer but worse UX                                           │
  │  - May block for long time on large directories                 │
  │                                                                 │
  │  Recommendation: Option A                                       │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Chunk Garbage Collection

  ┌─────────────────────────────────────────────────────────────────┐
  │                 TWO-PHASE CHUNK DELETION                        │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Phase 1: Metadata deletion                                     │
  │  - Remove chunk records from metadata DB                        │
  │  - Add chunk_ids to CHUNK_GC_QUEUE                              │
  │                                                                 │
  │  Phase 2: Physical deletion (delayed)                           │
  │  - Wait grace period (e.g., 24 hours)                           │
  │  - Allows recovery from accidental deletion                     │
  │  - Chunk servers delete physical data                           │
  │                                                                 │
  │  Timeline:                                                      │
  │  ├─────────┼──────────────────────┼─────────────────┤          │
  │  t=0       t=0+                   t=24h                         │
  │  User      Metadata               Physical                      │
  │  deletes   removed                data deleted                  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Summary

  | Step | Action                                   | Duration           |
  |------|------------------------------------------|--------------------|
  | 1    | Mark root as DELETED, remove from parent | ~10ms              |
  | 2    | Return success to client                 | Instant            |
  | 3    | Background: traverse tree in batches     | Minutes to hours   |
  | 4    | Background: delete chunk metadata        | Concurrent         |
  | 5    | Background: physical chunk deletion      | After grace period |

  Key Benefits:
  - User sees instant deletion
  - System remains consistent (deleted paths return "not found")
  - Failure recovery is built-in
  - No long-running transactions or locks

  Would you like me to discuss how to handle undo/restore of deleted directories, or dive into the chunk garbage collection process?

> How do you ensure the file list is always up-to-date immediately (Strong 
Consistency)? 

⏺ Ensuring Strong Consistency for LIST Operations

  The Problem

  ┌─────────────────────────────────────────────────────────────────┐
  │                    CONSISTENCY CHALLENGE                        │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Time ──────────────────────────────────────────────────►       │
  │                                                                 │
  │  Client A:  PUT /data/file.txt ─────► SUCCESS                  │
  │                                           │                     │
  │  Client B:                                │  LIST /data         │
  │                                           ▼                     │
  │                                      Should see file.txt!       │
  │                                                                 │
  │  If Client B reads from stale replica → file.txt missing       │
  │  This violates strong consistency requirement                   │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Sources of Staleness

  | Source           | Description                                   |
  |------------------|-----------------------------------------------|
  | Replica lag      | Raft followers haven't received latest commit |
  | Client cache     | Client cached old directory listing           |
  | Load balancer    | Routes read to outdated replica               |
  | In-flight writes | Write acknowledged but not yet visible        |

  ---
  Solution: Linearizable Reads

  Approach 1: Leader Reads (Simplest)

  ┌─────────────────────────────────────────────────────────────────┐
  │                    LEADER-ONLY READS                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  ALL reads go through Raft leader                               │
  │                                                                 │
  │      Client                                                     │
  │        │                                                        │
  │        ▼                                                        │
  │   ┌─────────┐    ┌─────────┐    ┌─────────┐                    │
  │   │ LEADER  │    │Follower │    │Follower │                    │
  │   │  ████   │    │         │    │         │                    │
  │   │  READ   │    │         │    │         │                    │
  │   └─────────┘    └─────────┘    └─────────┘                    │
  │                                                                 │
  │   Problem: Leader must confirm it's still leader!               │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Why leader must verify: Network partition could have caused a new leader election, and this node might be a stale ex-leader.

  Solution: ReadIndex Protocol

  ┌─────────────────────────────────────────────────────────────────┐
  │                   READINDEX PROTOCOL                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  1. Client sends LIST /data to Leader                           │
  │                                                                 │
  │  2. Leader records current commit index (e.g., index=100)       │
  │                                                                 │
  │  3. Leader sends heartbeat to majority of followers             │
  │     "Are you still following me?"                               │
  │                                                                 │
  │  4. Majority responds "Yes, you're our leader"                  │
  │                                                                 │
  │  5. Leader waits until local state machine applies index=100    │
  │                                                                 │
  │  6. Leader reads from state machine and returns result          │
  │                                                                 │
  │  Guarantee: Read sees all writes committed before step 2        │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Approach 2: Lease-Based Reads (Better Performance)

  ┌─────────────────────────────────────────────────────────────────┐
  │                    LEADER LEASE                                 │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Leader holds a "lease" - a time-bounded leadership guarantee   │
  │                                                                 │
  │  Timeline:                                                      │
  │  ├────────────────────────────────────────────────────────┤    │
  │  │◄──────────── Lease valid (e.g., 10 seconds) ──────────►│    │
  │  t=0                                                    t=10    │
  │  Lease                                                  Lease   │
  │  granted                                                expires │
  │                                                                 │
  │  During valid lease:                                            │
  │  - Leader can serve reads WITHOUT heartbeat confirmation        │
  │  - Much lower latency                                           │
  │                                                                 │
  │  Requirements:                                                  │
  │  - Clock synchronization (bounded drift)                        │
  │  - Followers won't elect new leader until lease expires        │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  function handleRead(request):
      if currentTime < leaseExpiry:
          # Safe to read locally without confirmation
          return readFromStateMachine(request)
      else:
          # Lease expired, use ReadIndex protocol
          return readIndexProtocol(request)

  ---
  Approach 3: Follower Reads with ReadIndex (Scale Reads)

  ┌─────────────────────────────────────────────────────────────────┐
  │                 FOLLOWER READ WITH READINDEX                    │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  1. Client sends LIST /data to Follower                         │
  │                                                                 │
  │  2. Follower asks Leader: "What's the current commit index?"    │
  │                                                                 │
  │  3. Leader responds: "Commit index = 100"                       │
  │     (Leader also confirms it's still leader via heartbeat)      │
  │                                                                 │
  │  4. Follower waits until it has applied index=100 locally       │
  │                                                                 │
  │  5. Follower reads from local state machine                     │
  │                                                                 │
  │      Client          Follower              Leader               │
  │        │                │                    │                  │
  │        │──LIST /data───►│                    │                  │
  │        │                │──ReadIndex req────►│                  │
  │        │                │◄─index=100─────────│                  │
  │        │                │                    │                  │
  │        │                │ (wait for apply)   │                  │
  │        │◄───result──────│                    │                  │
  │                                                                 │
  │  Benefit: Distributes read load across followers                │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Write Path: Ensuring Visibility

  Atomic Commit for File Uploads

  ┌─────────────────────────────────────────────────────────────────┐
  │              FILE NOT VISIBLE UNTIL COMMIT                      │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  PUT /data/file.txt (100MB file)                                │
  │                                                                 │
  │  Step 1: Create file record (status: UPLOADING)                 │
  │  ┌─────────────────────────────────────────────────────────┐   │
  │  │ inode_id=123, name="file.txt", status=UPLOADING         │   │
  │  └─────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │  LIST /data at this point → file.txt NOT shown                 │
  │  (Query: WHERE status = 'ACTIVE')                               │
  │                                                                 │
  │  Step 2: Upload chunks (may take minutes)                       │
  │                                                                 │
  │  Step 3: COMMIT - Update status atomically via Raft             │
  │  ┌─────────────────────────────────────────────────────────┐   │
  │  │ inode_id=123, name="file.txt", status=ACTIVE  ✓         │   │
  │  └─────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │  LIST /data at this point → file.txt SHOWN                     │
  │                                                                 │
  │  Only after Raft commits status=ACTIVE is file visible         │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Write Response Timing

  ┌─────────────────────────────────────────────────────────────────┐
  │            WHEN TO ACKNOWLEDGE WRITE SUCCESS                    │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Client                Leader              Followers            │
  │    │                     │                     │                │
  │    │──COMMIT file.txt───►│                     │                │
  │    │                     │                     │                │
  │    │                     │──Append to log─────►│                │
  │    │                     │                     │                │
  │    │                     │◄──ACK (majority)────│                │
  │    │                     │                     │                │
  │    │                     │ Apply to state      │                │
  │    │                     │ machine             │                │
  │    │                     │                     │                │
  │    │◄──SUCCESS───────────│                     │                │
  │    │                     │                     │                │
  │                                                                 │
  │  SUCCESS returned only AFTER:                                   │
  │  1. Majority of replicas have the log entry                     │
  │  2. Leader has applied to state machine                         │
  │                                                                 │
  │  Any subsequent read (even to different replica) will see it    │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Handling Client Caching

  ┌─────────────────────────────────────────────────────────────────┐
  │                   CLIENT CACHE INVALIDATION                     │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Option A: No client caching (simplest)                         │
  │  - Every LIST hits metadata service                             │
  │  - Always fresh, but high load                                  │
  │                                                                 │
  │  Option B: Version-based invalidation                           │
  │  - Each directory has a version number                          │
  │  - Client caches (path, version, contents)                      │
  │  - On LIST: send cached version, server returns:                │
  │    - "NOT_MODIFIED" if version unchanged                        │
  │    - New contents if version changed                            │
  │                                                                 │
  │  Option C: Watch/Subscribe (like etcd)                          │
  │  - Client subscribes to directory changes                       │
  │  - Server pushes updates in real-time                           │
  │  - Best for applications that need live updates                 │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Version-based cache example:

  # First request
  Client: LIST /data
  Server: {version: 5, files: [a.txt, b.txt]}
  Client: Cache {"/data": {version: 5, files: [a.txt, b.txt]}}

  # Later request
  Client: LIST /data (If-Version: 5)
  Server: (checks current version is 7)
  Server: {version: 7, files: [a.txt, b.txt, c.txt]}  # Full response

  # Or if unchanged:
  Client: LIST /data (If-Version: 7)
  Server: NOT_MODIFIED
  Client: Use cached result

  ---
  Summary: Consistency Guarantees

  | Operation    | Guarantee                       | How                           |
  |--------------|---------------------------------|-------------------------------|
  | CREATE       | Visible immediately after ACK   | Raft majority commit          |
  | DELETE       | Invisible immediately after ACK | Raft majority commit          |
  | PUT (upload) | Visible after COMMIT ACK        | Two-phase with status flag    |
  | LIST         | Sees all committed writes       | ReadIndex / Leader lease      |
  | GET          | Returns committed version       | Metadata read is linearizable |

  ---
  Performance vs Consistency Trade-offs

  | Approach             | Latency | Throughput           | Consistency           |
  |----------------------|---------|----------------------|-----------------------|
  | Leader reads only    | Higher  | Limited by leader    | Linearizable          |
  | Leader lease         | Low     | Limited by leader    | Linearizable          |
  | Follower ReadIndex   | Medium  | Scales with replicas | Linearizable          |
  | Follower stale reads | Lowest  | Highest              | Eventual (NOT for us) |

  Recommendation: Use Leader Lease for most reads, with Follower ReadIndex to scale read-heavy workloads while maintaining strong consistency.

  Would you like me to show code examples for implementing the ReadIndex protocol, or discuss how this interacts with the distributed locking mechanism?

> What did you sacrifice based on the CAP theorem? 

⏺ CAP Theorem Trade-off Analysis

  Quick CAP Recap

  ┌─────────────────────────────────────────────────────────────────┐
  │                      CAP THEOREM                                │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │                     Consistency (C)                             │
  │                          ▲                                      │
  │                         /│\                                     │
  │                        / │ \                                    │
  │                       /  │  \                                   │
  │                      /   │   \                                  │
  │                     /  PICK   \                                 │
  │                    /    TWO    \                                │
  │                   /      │      \                               │
  │                  /       │       \                              │
  │                 ▼        │        ▼                             │
  │         Availability ◄───┴───► Partition                        │
  │              (A)              Tolerance (P)                     │
  │                                                                 │
  │   Network partitions ARE inevitable → must choose P            │
  │   Real choice: CP or AP                                         │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Our Choice: CP (Consistency + Partition Tolerance)

  We Sacrificed: Availability

  ┌─────────────────────────────────────────────────────────────────┐
  │                                                                 │
  │   ┌─────────────┐              ┌─────────────┐                 │
  │   │             │              │             │                 │
  │   │  Consistency│  ✓ CHOSEN    │ Partition   │  ✓ CHOSEN      │
  │   │             │              │ Tolerance   │                 │
  │   └─────────────┘              └─────────────┘                 │
  │                                                                 │
  │                  ┌─────────────┐                                │
  │                  │             │                                │
  │                  │ Availability│  ✗ SACRIFICED                 │
  │                  │             │                                │
  │                  └─────────────┘                                │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  What "Sacrificing Availability" Means

  Scenario: Network Partition

  ┌─────────────────────────────────────────────────────────────────┐
  │                    NETWORK PARTITION                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Datacenter A                    Datacenter B                  │
  │   ┌─────────────────┐             ┌─────────────────┐          │
  │   │                 │             │                 │          │
  │   │  Node 1 (Leader)│             │  Node 4         │          │
  │   │  Node 2         │    ████     │  Node 5         │          │
  │   │  Node 3         │   NETWORK   │                 │          │
  │   │                 │   PARTITION │                 │          │
  │   │  (3 nodes)      │             │  (2 nodes)      │          │
  │   │  = MAJORITY     │             │  = MINORITY     │          │
  │   └─────────────────┘             └─────────────────┘          │
  │                                                                 │
  │   Clients in A: ✓ Can read and write (majority available)      │
  │   Clients in B: ✗ UNAVAILABLE (cannot reach majority)          │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  What Happens to Minority Partition

  ┌─────────────────────────────────────────────────────────────────┐
  │              MINORITY PARTITION BEHAVIOR                        │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Client in Datacenter B tries to:                              │
  │                                                                 │
  │   WRITE /data/file.txt                                          │
  │   ────────────────────                                          │
  │   → ERROR: "Cannot reach quorum, write rejected"               │
  │   → Client must retry or fail                                   │
  │                                                                 │
  │   LIST /data (linearizable)                                     │
  │   ─────────────────────────                                     │
  │   → ERROR: "Cannot confirm leader, read rejected"              │
  │   → Refuses to return potentially stale data                    │
  │                                                                 │
  │   WHY? Because serving requests could violate consistency:      │
  │   - Write might conflict with writes in majority partition      │
  │   - Read might return stale data (split brain)                  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Why We Made This Choice

  | Requirement                          | Why CP is necessary              |
  |--------------------------------------|----------------------------------|
  | "LIST must show changes immediately" | Cannot serve stale reads         |
  | "No last-writer-wins"                | Cannot accept conflicting writes |
  | File system semantics                | Users expect POSIX-like behavior |

  The Alternative (AP) Would Break Requirements

  ┌─────────────────────────────────────────────────────────────────┐
  │          IF WE CHOSE AP (Like S3, Cassandra, DynamoDB)         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Scenario: Network partition between datacenters               │
  │                                                                 │
  │   Datacenter A:                  Datacenter B:                  │
  │   Client writes                  Client writes                  │
  │   /data/config.txt = "v1"        /data/config.txt = "v2"       │
  │        │                              │                         │
  │        ▼                              ▼                         │
  │   Both succeed! ✓                Both succeed! ✓               │
  │                                                                 │
  │   After partition heals:                                        │
  │   Which version wins? → "Last writer wins" or conflict         │
  │                                                                 │
  │   Client reads /data/config.txt                                 │
  │   → Might get "v1" or "v2" depending on which replica          │
  │                                                                 │
  │   ❌ VIOLATES: "No last-writer-wins" requirement               │
  │   ❌ VIOLATES: "LIST must show changes immediately"            │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Real-World Impact

  When Availability is Sacrificed

  | Situation           | Impact                          | Mitigation               |
  |---------------------|---------------------------------|--------------------------|
  | Network partition   | Minority side unavailable       | Deploy across 3+ zones   |
  | Leader election     | Brief unavailability (~seconds) | Fast election timeout    |
  | Leader failure      | Writes blocked until new leader | Automatic failover       |
  | Majority nodes down | Complete outage                 | Proper capacity planning |

  Typical Unavailability Duration

  ┌─────────────────────────────────────────────────────────────────┐
  │                 UNAVAILABILITY WINDOWS                          │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Event                          Duration                       │
  │   ──────────────────────────────────────────────                │
  │   Leader election                ~1-5 seconds                   │
  │   Node restart                   ~seconds (catch up)            │
  │   Network partition              Until partition heals          │
  │   Datacenter failure (minority)  Until failover or recovery     │
  │                                                                 │
  │   In practice: 99.9%+ availability achievable with CP          │
  │   (partitions are rare in well-designed networks)              │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Comparison with Other Systems

  | System         | Choice       | Trade-off                                |
  |----------------|--------------|------------------------------------------|
  | Our DFS        | CP           | Unavailable during partition (minority)  |
  | etcd           | CP           | Same - used for Kubernetes coordination  |
  | ZooKeeper      | CP           | Same - configuration management          |
  | Cassandra      | AP           | Eventual consistency, conflicts possible |
  | DynamoDB       | AP (default) | Eventual consistency, can opt for strong |
  | S3             | AP           | Eventual consistency for LIST            |
  | Google Spanner | CP           | Uses TrueTime for global consistency     |
  | CockroachDB    | CP           | Serializable transactions                |

  ---
  Maximizing Availability Within CP

  Even though we sacrifice availability during partitions, we can minimize impact:

  ┌─────────────────────────────────────────────────────────────────┐
  │              AVAILABILITY OPTIMIZATIONS                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   1. Deploy across 3+ availability zones                        │
  │      - Single zone failure doesn't lose majority               │
  │                                                                 │
  │   2. Fast leader election                                       │
  │      - Raft election timeout: 150-300ms                         │
  │      - Minimize unavailability window                           │
  │                                                                 │
  │   3. Leader lease for reads                                     │
  │      - Reads don't need round-trip during valid lease          │
  │                                                                 │
  │   4. Follower reads with ReadIndex                              │
  │      - Distribute read load, reduce leader bottleneck          │
  │                                                                 │
  │   5. Client retry with backoff                                  │
  │      - Transient failures handled gracefully                    │
  │                                                                 │
  │   6. Health-aware load balancing                                │
  │      - Route away from unhealthy nodes quickly                  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Summary

  ┌─────────────────────────────────────────────────────────────────┐
  │                      OUR CAP DECISION                           │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   CHOSE:      Consistency + Partition Tolerance (CP)            │
  │                                                                 │
  │   SACRIFICED: Availability during network partitions            │
  │                                                                 │
  │   WHY:        Strong consistency is #1 requirement              │
  │               - LIST must reflect all committed writes          │
  │               - No conflicting writes allowed                   │
  │               - File system semantics expected                  │
  │                                                                 │
  │   IMPACT:     Minority partition becomes unavailable            │
  │               (but majority continues operating)                │
  │                                                                 │
  │   MITIGATION: Multi-zone deployment, fast failover,             │
  │               client retries, proper capacity planning          │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Would you like to discuss strategies for handling partial availability, or explore how systems like Google Spanner achieve both strong consistency and high availability through TrueTime?

> What do you do if a file is too big?
How do you split the file into pieces (chunking)?
How do you track the details of each chunk? 

⏺ Large File Handling: Chunking Strategy

  Why Chunking?

  ┌─────────────────────────────────────────────────────────────────┐
  │                    PROBLEMS WITH LARGE FILES                    │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Without chunking (storing file as single blob):               │
  │                                                                 │
  │   ❌ Memory: Can't load 100GB file into memory                 │
  │   ❌ Network: Single connection bottleneck                      │
  │   ❌ Failure: Must restart entire upload if connection drops   │
  │   ❌ Storage: Can't spread across multiple servers             │
  │   ❌ Replication: Must copy entire file for each replica       │
  │                                                                 │
  │   With chunking:                                                │
  │                                                                 │
  │   ✓ Stream chunks without full file in memory                  │
  │   ✓ Parallel uploads across multiple connections               │
  │   ✓ Resume from last successful chunk                          │
  │   ✓ Distribute chunks across cluster                           │
  │   ✓ Replicate individual chunks independently                  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Chunk Size Selection

  | Size            | Pros                                    | Cons                                        |
  |-----------------|-----------------------------------------|---------------------------------------------|
  | Small (1-4 MB)  | Fine-grained parallelism, fast recovery | High metadata overhead                      |
  | Medium (64 MB)  | Balanced trade-off                      | Good for most workloads                     |
  | Large (256 MB+) | Low metadata overhead                   | Slow recovery, wasted space for small files |

  Our Choice: 64 MB (Configurable)

  ┌─────────────────────────────────────────────────────────────────┐
  │                    CHUNK SIZE: 64 MB                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Why 64 MB?                                                    │
  │                                                                 │
  │   1. Metadata efficiency                                        │
  │      - 1 TB file = 16,384 chunks (manageable)                   │
  │      - vs 1 MB chunks = 1,048,576 chunks (too many!)           │
  │                                                                 │
  │   2. Network efficiency                                         │
  │      - Large enough to saturate network connection              │
  │      - Amortizes connection setup overhead                      │
  │                                                                 │
  │   3. Parallel benefit                                           │
  │      - 1 GB file = 16 chunks = 16 parallel uploads             │
  │                                                                 │
  │   4. Recovery cost                                              │
  │      - Failed chunk = retry 64 MB (acceptable)                  │
  │                                                                 │
  │   Reference: GFS uses 64 MB, HDFS uses 128 MB                   │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  How Chunking Works

  Step 1: Client-Side Splitting

  ┌─────────────────────────────────────────────────────────────────┐
  │                    FILE SPLITTING                               │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Original file: video.mp4 (256 MB)                             │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░│   │
  │   └────────────────────────────────────────────────────────┘   │
  │                           │                                     │
  │                           ▼  Split by 64 MB boundaries          │
  │   ┌──────────────┬──────────────┬──────────────┬──────────────┐│
  │   │   Chunk 0    │   Chunk 1    │   Chunk 2    │   Chunk 3    ││
  │   │    64 MB     │    64 MB     │    64 MB     │    64 MB     ││
  │   │ bytes 0-64M  │bytes 64M-128M│bytes128M-192M│bytes192M-256M││
  │   └──────────────┴──────────────┴──────────────┴──────────────┘│
  │                                                                 │
  │   Last chunk may be smaller (if file size not divisible)        │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Step 2: Compute Chunk Metadata

  # Client-side chunking logic
  CHUNK_SIZE = 64 * 1024 * 1024  # 64 MB

  def chunk_file(file_path):
      chunks = []
      file_size = os.path.getsize(file_path)

      with open(file_path, 'rb') as f:
          chunk_index = 0

          while True:
              data = f.read(CHUNK_SIZE)
              if not data:
                  break

              chunk = {
                  'index': chunk_index,
                  'offset': chunk_index * CHUNK_SIZE,
                  'size': len(data),
                  'checksum': sha256(data).hexdigest(),
                  'data': data
              }
              chunks.append(chunk)
              chunk_index += 1

      return chunks

  # Example output for 256 MB file:
  # [
  #   {index: 0, offset: 0,         size: 67108864, checksum: "a1b2c3..."},
  #   {index: 1, offset: 67108864,  size: 67108864, checksum: "d4e5f6..."},
  #   {index: 2, offset: 134217728, size: 67108864, checksum: "g7h8i9..."},
  #   {index: 3, offset: 201326592, size: 67108864, checksum: "j0k1l2..."},
  # ]

  ---
  Chunk Metadata Tracking

  Database Schema

  ┌─────────────────────────────────────────────────────────────────┐
  │                      CHUNK METADATA                             │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  FILE TABLE (INODE)                                             │
  │  ┌───────────────────────────────────────────────────────────┐ │
  │  │ inode_id │ name      │ size       │ chunk_size │ status   │ │
  │  ├───────────────────────────────────────────────────────────┤ │
  │  │ 12345    │ video.mp4 │ 268435456  │ 67108864   │ ACTIVE   │ │
  │  │          │           │ (256 MB)   │ (64 MB)    │          │ │
  │  └───────────────────────────────────────────────────────────┘ │
  │                                                                 │
  │  CHUNK TABLE                                                    │
  │  ┌───────────────────────────────────────────────────────────┐ │
  │  │chunk_id │inode_id│index│offset    │size    │checksum     │ │
  │  ├───────────────────────────────────────────────────────────┤ │
  │  │ uuid-1  │ 12345  │ 0   │ 0        │67108864│ a1b2c3...   │ │
  │  │ uuid-2  │ 12345  │ 1   │ 67108864 │67108864│ d4e5f6...   │ │
  │  │ uuid-3  │ 12345  │ 2   │134217728 │67108864│ g7h8i9...   │ │
  │  │ uuid-4  │ 12345  │ 3   │201326592 │67108864│ j0k1l2...   │ │
  │  └───────────────────────────────────────────────────────────┘ │
  │                                                                 │
  │  CHUNK_LOCATION TABLE (where replicas live)                     │
  │  ┌───────────────────────────────────────────────────────────┐ │
  │  │ chunk_id │ server_id │ status  │ created_at               │ │
  │  ├───────────────────────────────────────────────────────────┤ │
  │  │ uuid-1   │ server-A  │ ACTIVE  │ 2024-01-15 10:00:00      │ │
  │  │ uuid-1   │ server-B  │ ACTIVE  │ 2024-01-15 10:00:01      │ │
  │  │ uuid-1   │ server-C  │ ACTIVE  │ 2024-01-15 10:00:02      │ │
  │  │ uuid-2   │ server-D  │ ACTIVE  │ 2024-01-15 10:00:03      │ │
  │  │ uuid-2   │ server-E  │ ACTIVE  │ 2024-01-15 10:00:04      │ │
  │  │ ...      │ ...       │ ...     │ ...                      │ │
  │  └───────────────────────────────────────────────────────────┘ │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  RocksDB Key Design

  ┌─────────────────────────────────────────────────────────────────┐
  │                    ROCKSDB KEY PATTERNS                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Chunk by file (for listing all chunks of a file):              │
  │  Key:   chunk:{inode_id}:{chunk_index}                         │
  │  Value: {chunk_id, size, checksum}                             │
  │                                                                 │
  │  Example:                                                       │
  │  chunk:12345:0 → {id: "uuid-1", size: 67108864, ...}          │
  │  chunk:12345:1 → {id: "uuid-2", size: 67108864, ...}          │
  │  chunk:12345:2 → {id: "uuid-3", size: 67108864, ...}          │
  │                                                                 │
  │  PrefixScan("chunk:12345:") → all chunks for file 12345        │
  │                                                                 │
  │  ─────────────────────────────────────────────────────────────  │
  │                                                                 │
  │  Chunk locations (for finding servers):                         │
  │  Key:   loc:{chunk_id}:{server_id}                             │
  │  Value: {status, created_at}                                   │
  │                                                                 │
  │  Example:                                                       │
  │  loc:uuid-1:server-A → {status: ACTIVE, ...}                   │
  │  loc:uuid-1:server-B → {status: ACTIVE, ...}                   │
  │  loc:uuid-1:server-C → {status: ACTIVE, ...}                   │
  │                                                                 │
  │  PrefixScan("loc:uuid-1:") → all servers holding uuid-1        │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Complete Upload Flow

  ┌─────────────────────────────────────────────────────────────────┐
  │                    CHUNKED UPLOAD FLOW                          │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Client              Metadata Service           Chunk Servers   │
  │    │                       │                          │         │
  │    │ 1. InitUpload         │                          │         │
  │    │   path=/data/video.mp4│                          │         │
  │    │   size=256MB          │                          │         │
  │    │──────────────────────►│                          │         │
  │    │                       │                          │         │
  │    │                       │ 2. Create inode          │         │
  │    │                       │    status=UPLOADING      │         │
  │    │                       │    Allocate 4 chunk IDs  │         │
  │    │                       │    Select servers        │         │
  │    │                       │                          │         │
  │    │ 3. UploadPlan         │                          │         │
  │    │   upload_id=xyz       │                          │         │
  │    │   chunks=[            │                          │         │
  │    │     {idx:0, id:uuid-1,│                          │         │
  │    │      servers:[A,B,C]} │                          │         │
  │    │     {idx:1, id:uuid-2,│                          │         │
  │    │      servers:[D,E,F]} │                          │         │
  │    │     ...               │                          │         │
  │    │   ]                   │                          │         │
  │    │◄──────────────────────│                          │         │
  │    │                       │                          │         │
  │    │ 4. Upload chunks in parallel                     │         │
  │    │─────────────────────────────────────────────────►│         │
  │    │   Chunk 0 → Server A (primary)                  │         │
  │    │   Chunk 1 → Server D (primary)                  │         │
  │    │   Chunk 2 → Server G (primary)                  │         │
  │    │   Chunk 3 → Server J (primary)                  │         │
  │    │                       │                          │         │
  │    │                       │          5. Primary replicates     │
  │    │                       │             to secondaries         │
  │    │                       │             A→B,C  D→E,F  etc     │
  │    │                       │                          │         │
  │    │ 6. All chunks ACK     │                          │         │
  │    │◄────────────────────────────────────────────────│         │
  │    │                       │                          │         │
  │    │ 7. CommitUpload       │                          │         │
  │    │   upload_id=xyz       │                          │         │
  │    │   checksums=[...]     │                          │         │
  │    │──────────────────────►│                          │         │
  │    │                       │                          │         │
  │    │                       │ 8. Verify all chunks     │         │
  │    │                       │    Update status=ACTIVE  │         │
  │    │                       │    Commit via Raft       │         │
  │    │                       │                          │         │
  │    │ 9. Success            │                          │         │
  │    │◄──────────────────────│                          │         │
  │    │                       │                          │         │
  │    │  File now visible in LIST                        │         │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Complete Download Flow

  ┌─────────────────────────────────────────────────────────────────┐
  │                   CHUNKED DOWNLOAD FLOW                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Client              Metadata Service           Chunk Servers   │
  │    │                       │                          │         │
  │    │ 1. GET /data/video.mp4│                          │         │
  │    │──────────────────────►│                          │         │
  │    │                       │                          │         │
  │    │                       │ 2. Lookup inode          │         │
  │    │                       │    Fetch chunk list      │         │
  │    │                       │    Get server locations  │         │
  │    │                       │                          │         │
  │    │ 3. FileMetadata       │                          │         │
  │    │   size=256MB          │                          │         │
  │    │   chunks=[            │                          │         │
  │    │     {idx:0, id:uuid-1,│                          │         │
  │    │      checksum:a1b2c3, │                          │         │
  │    │      servers:[A,B,C]} │                          │         │
  │    │     {idx:1, id:uuid-2,│                          │         │
  │    │      checksum:d4e5f6, │                          │         │
  │    │      servers:[D,E,F]} │                          │         │
  │    │     ...               │                          │         │
  │    │   ]                   │                          │         │
  │    │◄──────────────────────│                          │         │
  │    │                       │                          │         │
  │    │ 4. Download chunks in parallel                   │         │
  │    │─────────────────────────────────────────────────►│         │
  │    │   Get chunk 0 from A (or B,C if A fails)        │         │
  │    │   Get chunk 1 from D (or E,F if D fails)        │         │
  │    │   Get chunk 2 from G                             │         │
  │    │   Get chunk 3 from J                             │         │
  │    │                       │                          │         │
  │    │ 5. Receive chunks     │                          │         │
  │    │◄────────────────────────────────────────────────│         │
  │    │                       │                          │         │
  │    │ 6. Verify checksums   │                          │         │
  │    │    Reassemble file    │                          │         │
  │    │    Write to local disk│                          │         │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Chunk Replication Strategy

  ┌─────────────────────────────────────────────────────────────────┐
  │                  CHUNK PLACEMENT STRATEGY                       │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Goals:                                                        │
  │   1. Spread replicas across failure domains (racks/zones)       │
  │   2. Balance storage utilization                                │
  │   3. Minimize network distance for reads                        │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │                     Rack 1                              │  │
  │   │   ┌─────────┐  ┌─────────┐  ┌─────────┐               │  │
  │   │   │Server A │  │Server B │  │Server C │               │  │
  │   │   │ C0[P]   │  │ C1[S]   │  │ C2[S]   │               │  │
  │   │   │ C3[S]   │  │         │  │ C0[S]   │               │  │
  │   │   └─────────┘  └─────────┘  └─────────┘               │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │                     Rack 2                              │  │
  │   │   ┌─────────┐  ┌─────────┐  ┌─────────┐               │  │
  │   │   │Server D │  │Server E │  │Server F │               │  │
  │   │   │ C1[P]   │  │ C0[S]   │  │ C3[S]   │               │  │
  │   │   │ C2[S]   │  │ C3[P]   │  │ C1[S]   │               │  │
  │   │   └─────────┘  └─────────┘  └─────────┘               │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   [P] = Primary replica    [S] = Secondary replica              │
  │                                                                 │
  │   Each chunk has 3 replicas across different racks              │
  │   Survives: any 2 server failures OR entire rack failure        │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Handling Edge Cases

  Resumable Uploads

  ┌─────────────────────────────────────────────────────────────────┐
  │                    RESUMABLE UPLOAD                             │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Upload interrupted at chunk 2 of 4:                           │
  │                                                                 │
  │   Client state (persisted locally):                             │
  │   {                                                             │
  │     upload_id: "xyz",                                           │
  │     file_path: "/data/video.mp4",                               │
  │     completed_chunks: [0, 1],  # Already uploaded               │
  │     pending_chunks: [2, 3]     # Still need to upload          │
  │   }                                                             │
  │                                                                 │
  │   Resume flow:                                                  │
  │   1. Client: ResumeUpload(upload_id="xyz")                      │
  │   2. Server: Returns status of each chunk                       │
  │   3. Client: Uploads only chunks 2 and 3                        │
  │   4. Client: CommitUpload                                       │
  │                                                                 │
  │   Server also tracks upload state (with expiration):            │
  │   - Incomplete uploads expire after 24 hours                    │
  │   - Orphaned chunks garbage collected                           │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Checksum Verification

  ┌─────────────────────────────────────────────────────────────────┐
  │                  INTEGRITY VERIFICATION                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   On Upload:                                                    │
  │   Client ──chunk data──► Server                                │
  │           ──checksum───►                                       │
  │                          Server computes checksum               │
  │                          If mismatch → reject, client retries  │
  │                                                                 │
  │   On Download:                                                  │
  │   Server ──chunk data──► Client                                │
  │          ──checksum────►                                       │
  │                          Client verifies checksum               │
  │                          If mismatch → fetch from another replica│
  │                                                                 │
  │   Background scrubbing:                                         │
  │   - Chunk servers periodically verify stored chunks             │
  │   - Corrupted chunks reported to metadata service               │
  │   - Re-replicated from healthy replica                          │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Small Files Optimization

  ┌─────────────────────────────────────────────────────────────────┐
  │                  SMALL FILE HANDLING                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Problem: 1 KB file in 64 MB chunk = wasted space              │
  │                                                                 │
  │   Solution 1: Inline small files in metadata                    │
  │   - Files < 64 KB stored directly in inode record               │
  │   - No chunks created                                           │
  │   - Fast read (single metadata lookup)                          │
  │                                                                 │
  │   Solution 2: Block packing                                     │
  │   - Multiple small files packed into single chunk               │
  │   - More complex but better space efficiency                    │
  │                                                                 │
  │   Recommendation: Inline for files < 64 KB                      │
  │                                                                 │
  │   INODE with inline data:                                       │
  │   {                                                             │
  │     inode_id: 12345,                                            │
  │     name: "config.json",                                        │
  │     size: 1024,                                                 │
  │     storage_type: "INLINE",                                     │
  │     inline_data: "eyJrZXkiOiAidmFsdWUi...}" # base64          │
  │   }                                                             │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Summary

  | Aspect      | Decision                                    |
  |-------------|---------------------------------------------|
  | Chunk size  | 64 MB (configurable)                        |
  | Splitting   | Client-side, by fixed boundaries            |
  | Chunk ID    | UUID (globally unique)                      |
  | Tracking    | Metadata service (inode → chunks → servers) |
  | Replication | 3 replicas across failure domains           |
  | Integrity   | SHA-256 checksum per chunk                  |
  | Small files | Inline in metadata (< 64 KB)                |
  | Resumable   | Track completed chunks, resume remaining    |

  Would you like me to discuss how to handle file updates (modifying a chunk in the middle of a file) or dive into the chunk server storage implementation?

> File Immutability

Why can't files be changed directly (why are they immutable)?
How do you handle different versions of the same file?
How can a user download a file while someone else is updating it? 

⏺ File Immutability & Versioning

  Why Immutability?

  ┌─────────────────────────────────────────────────────────────────┐
  │                 MUTABLE vs IMMUTABLE FILES                      │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   MUTABLE (Traditional approach):                               │
  │   ┌─────────────────────────────────────────┐                  │
  │   │  file.txt                               │                  │
  │   │  ┌───────┬───────┬───────┬───────┐     │                  │
  │   │  │Chunk 0│Chunk 1│Chunk 2│Chunk 3│     │                  │
  │   │  └───────┴───────┴───────┴───────┘     │                  │
  │   │      │                                  │                  │
  │   │      ▼ Modify chunk 1 in-place          │                  │
  │   │  ┌───────┬───────┬───────┬───────┐     │                  │
  │   │  │Chunk 0│CHANGED│Chunk 2│Chunk 3│     │                  │
  │   │  └───────┴───────┴───────┴───────┘     │                  │
  │   └─────────────────────────────────────────┘                  │
  │                                                                 │
  │   IMMUTABLE (Our approach):                                     │
  │   ┌─────────────────────────────────────────┐                  │
  │   │  file.txt v1                            │                  │
  │   │  ┌───────┬───────┬───────┬───────┐     │                  │
  │   │  │Chunk 0│Chunk 1│Chunk 2│Chunk 3│     │                  │
  │   │  └───────┴───────┴───────┴───────┘     │                  │
  │   │                                         │                  │
  │   │  file.txt v2 (new version created)      │                  │
  │   │  ┌───────┬───────┬───────┬───────┐     │                  │
  │   │  │Chunk 0│NEW C1 │Chunk 2│Chunk 3│     │                  │
  │   │  └───────┴───────┴───────┴───────┘     │                  │
  │   │      ▲               ▲       ▲          │                  │
  │   │      └───────────────┴───────┘          │                  │
  │   │         Reuse unchanged chunks          │                  │
  │   └─────────────────────────────────────────┘                  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Benefits of Immutability

  | Problem with Mutable                                       | How Immutability Solves It                                    |
  |------------------------------------------------------------|---------------------------------------------------------------|
  | Partial writes - Crash mid-update leaves corrupted file    | New version atomic - old version intact until commit          |
  | Reader sees torn data - Reading while write in progress    | Readers see complete old version OR complete new version      |
  | Replication complexity - Must sync partial changes         | Chunks never change - replicate once, done forever            |
  | Caching invalidation - When does cache become stale?       | Chunk content keyed by ID - same ID = same content forever    |
  | Backup/snapshot complexity - Point-in-time state           | Just record version pointer - chunks already immutable        |
  | Concurrent write conflicts - Two writers modify same chunk | Each creates new version - conflict resolved at version level |

  ---
  Versioning Design

  Version Metadata Schema

  ┌─────────────────────────────────────────────────────────────────┐
  │                    VERSION TRACKING                             │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  INODE TABLE (file identity - stable)                           │
  │  ┌───────────────────────────────────────────────────────────┐ │
  │  │ inode_id │ name      │ current_version │ created_at       │ │
  │  ├───────────────────────────────────────────────────────────┤ │
  │  │ 12345    │ doc.pdf   │ 3               │ 2024-01-01       │ │
  │  └───────────────────────────────────────────────────────────┘ │
  │                             │                                   │
  │                             ▼                                   │
  │  VERSION TABLE (each version is a snapshot)                     │
  │  ┌───────────────────────────────────────────────────────────┐ │
  │  │ inode_id │ version │ size     │ created_at  │ status      │ │
  │  ├───────────────────────────────────────────────────────────┤ │
  │  │ 12345    │ 1       │ 10485760 │ 2024-01-01  │ ARCHIVED    │ │
  │  │ 12345    │ 2       │ 10485800 │ 2024-01-05  │ ARCHIVED    │ │
  │  │ 12345    │ 3       │ 10486000 │ 2024-01-10  │ CURRENT     │ │
  │  └───────────────────────────────────────────────────────────┘ │
  │                             │                                   │
  │                             ▼                                   │
  │  VERSION_CHUNKS TABLE (chunks for each version)                 │
  │  ┌───────────────────────────────────────────────────────────┐ │
  │  │ inode_id │ version │ chunk_idx │ chunk_id  │ checksum     │ │
  │  ├───────────────────────────────────────────────────────────┤ │
  │  │ 12345    │ 1       │ 0         │ uuid-A    │ abc123...    │ │
  │  │ 12345    │ 1       │ 1         │ uuid-B    │ def456...    │ │
  │  │ 12345    │ 2       │ 0         │ uuid-A    │ abc123... ◄──reused │
  │  │ 12345    │ 2       │ 1         │ uuid-C    │ ghi789...    │ │
  │  │ 12345    │ 3       │ 0         │ uuid-A    │ abc123... ◄──reused │
  │  │ 12345    │ 3       │ 1         │ uuid-D    │ jkl012...    │ │
  │  └───────────────────────────────────────────────────────────┘ │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Chunk Deduplication

  ┌─────────────────────────────────────────────────────────────────┐
  │                   CHUNK REUSE ACROSS VERSIONS                   │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Version 1: 4 chunks                                           │
  │   ┌─────────┬─────────┬─────────┬─────────┐                    │
  │   │ uuid-A  │ uuid-B  │ uuid-C  │ uuid-D  │                    │
  │   │ "hello" │ "world" │ "foo"   │ "bar"   │                    │
  │   └─────────┴─────────┴─────────┴─────────┘                    │
  │                                                                 │
  │   Version 2: Only chunk 1 changed                               │
  │   ┌─────────┬─────────┬─────────┬─────────┐                    │
  │   │ uuid-A  │ uuid-E  │ uuid-C  │ uuid-D  │                    │
  │   │ "hello" │ "EARTH" │ "foo"   │ "bar"   │                    │
  │   └─────────┴─────────┴─────────┴─────────┘                    │
  │       ▲         │         ▲         ▲                           │
  │       │         │         │         │                           │
  │     REUSED   NEW CHUNK  REUSED    REUSED                        │
  │                                                                 │
  │   Storage used: 5 unique chunks (not 8)                         │
  │                                                                 │
  │   Reference counting:                                           │
  │   uuid-A: refcount=2 (used by v1 and v2)                       │
  │   uuid-B: refcount=1 (only v1)                                 │
  │   uuid-C: refcount=2                                            │
  │   uuid-D: refcount=2                                            │
  │   uuid-E: refcount=1 (only v2)                                 │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  File Update Flow

  How "Update" Works with Immutability

  ┌─────────────────────────────────────────────────────────────────┐
  │                    FILE UPDATE FLOW                             │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │  Client              Metadata Service           Chunk Servers   │
  │    │                       │                          │         │
  │    │ 1. UpdateFile         │                          │         │
  │    │    path=/doc.pdf      │                          │         │
  │    │    base_version=2     │                          │         │
  │    │──────────────────────►│                          │         │
  │    │                       │                          │         │
  │    │                       │ 2. Create new version    │         │
  │    │                       │    version=3             │         │
  │    │                       │    status=UPLOADING      │         │
  │    │                       │                          │         │
  │    │ 3. Return upload plan │                          │         │
  │    │    version=3          │                          │         │
  │    │    existing_chunks=   │                          │         │
  │    │      [uuid-A, uuid-C, │                          │         │
  │    │       uuid-D]         │                          │         │
  │    │◄──────────────────────│                          │         │
  │    │                       │                          │         │
  │    │ 4. Client computes which chunks changed          │         │
  │    │    Chunk 0: checksum matches uuid-A → skip      │         │
  │    │    Chunk 1: checksum differs → upload new       │         │
  │    │    Chunk 2: checksum matches uuid-C → skip      │         │
  │    │    Chunk 3: checksum matches uuid-D → skip      │         │
  │    │                       │                          │         │
  │    │ 5. Upload only changed chunk                     │         │
  │    │─────────────────────────────────────────────────►│         │
  │    │    new chunk uuid-E                              │         │
  │    │                       │                          │         │
  │    │ 6. CommitVersion      │                          │         │
  │    │    version=3          │                          │         │
  │    │    chunks=[uuid-A,    │                          │         │
  │    │      uuid-E, uuid-C,  │                          │         │
  │    │      uuid-D]          │                          │         │
  │    │──────────────────────►│                          │         │
  │    │                       │                          │         │
  │    │                       │ 7. Verify chunks exist   │         │
  │    │                       │    Update current_version│         │
  │    │                       │    Increment refcounts   │         │
  │    │                       │    Commit via Raft       │         │
  │    │                       │                          │         │
  │    │ 8. Success            │                          │         │
  │    │◄──────────────────────│                          │         │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Concurrent Read During Update

  The Core Problem

  ┌─────────────────────────────────────────────────────────────────┐
  │                  CONCURRENT ACCESS SCENARIO                     │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Time ────────────────────────────────────────────────────►    │
  │                                                                 │
  │   Writer:  [Start Update v1→v2]─────────────────[Commit v2]    │
  │                    │                                  │         │
  │   Reader A:        │  [Start Read]────[Download]      │         │
  │                    │         │             │          │         │
  │   Reader B:        │         │             │    [Start Read]──► │
  │                    │         │             │          │         │
  │                                                                 │
  │   Questions:                                                    │
  │   - Reader A started during update: which version?              │
  │   - Reader B started after commit: which version?               │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Solution: Version Pinning

  ┌─────────────────────────────────────────────────────────────────┐
  │                    VERSION PINNING                              │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Rule: Reader gets a consistent version at read start          │
  │                                                                 │
  │   Time ────────────────────────────────────────────────────►    │
  │                                                                 │
  │   Current version:  [───── v1 ─────────────][────── v2 ──────]  │
  │                                             │                   │
  │                                           Commit                │
  │                                                                 │
  │   Reader A: Starts at t1 (v1 is current)                        │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │ 1. GET /doc.pdf                                        │   │
  │   │ 2. Server: current_version = 1                         │   │
  │   │ 3. Return: version=1, chunks=[uuid-A, uuid-B, ...]    │   │
  │   │ 4. Reader downloads chunks for v1                      │   │
  │   │ 5. Even if v2 commits during download, reader gets v1  │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │   Reader B: Starts at t2 (v2 is current)                        │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │ 1. GET /doc.pdf                                        │   │
  │   │ 2. Server: current_version = 2                         │   │
  │   │ 3. Return: version=2, chunks=[uuid-A, uuid-E, ...]    │   │
  │   │ 4. Reader downloads chunks for v2                      │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │   Both readers get CONSISTENT data (complete version)           │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Why This Works

  ┌─────────────────────────────────────────────────────────────────┐
  │                 IMMUTABILITY GUARANTEES                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   1. Chunks are never modified                                  │
  │      - uuid-A always contains same data                         │
  │      - Safe to download anytime                                 │
  │                                                                 │
  │   2. Version pointer is atomic                                  │
  │      - current_version switches from 1 to 2 instantly           │
  │      - No intermediate state                                    │
  │                                                                 │
  │   3. Old chunks are retained while referenced                   │
  │      - uuid-B (only in v1) stays until Reader A finishes        │
  │      - Reference counting prevents premature deletion           │
  │                                                                 │
  │   Reader experience:                                            │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ ✓ Always sees complete, consistent file                 │  │
  │   │ ✓ Never sees partial update                             │  │
  │   │ ✓ Never sees mixed chunks from different versions       │  │
  │   │ ✓ Download can take arbitrarily long - chunks won't     │  │
  │   │   disappear mid-download                                │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Version Lifecycle & Garbage Collection

  ┌─────────────────────────────────────────────────────────────────┐
  │                   VERSION LIFECYCLE                             │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Version States:                                               │
  │                                                                 │
  │   UPLOADING ──► CURRENT ──► ARCHIVED ──► DELETED               │
  │       │             │           │            │                  │
  │       │             │           │            ▼                  │
  │       │             │           │      Chunks dereferenced      │
  │       │             │           │      GC if refcount=0         │
  │       │             │           │                               │
  │       │             │           └── Keep N versions (policy)    │
  │       │             │                                           │
  │       │             └── New version committed                   │
  │       │                                                         │
  │       └── Upload failed/abandoned → cleanup                    │
  │                                                                 │
  │   Example retention policy:                                     │
  │   - Keep last 10 versions                                       │
  │   - Keep versions from last 30 days                             │
  │   - Keep at least 1 version always                              │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Reference Counting for Chunk GC

  ┌─────────────────────────────────────────────────────────────────┐
  │                  CHUNK REFERENCE COUNTING                       │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   CHUNK_REFS TABLE                                              │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ chunk_id │ refcount │ last_referenced                   │  │
  │   ├─────────────────────────────────────────────────────────┤  │
  │   │ uuid-A   │ 3        │ 2024-01-10                        │  │
  │   │ uuid-B   │ 1        │ 2024-01-01  ◄── only v1 uses     │  │
  │   │ uuid-C   │ 2        │ 2024-01-05                        │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   When version 1 is deleted:                                    │
  │   1. Decrement refcount for each chunk in v1                    │
  │   2. uuid-B refcount: 1→0                                      │
  │   3. uuid-B added to GC queue                                   │
  │   4. Chunk servers delete uuid-B after grace period             │
  │                                                                 │
  │   Active reads protection:                                      │
  │   - Track in-flight downloads                                   │
  │   - Don't GC chunks with active readers                        │
  │   - Or: grace period long enough for downloads to complete      │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Conflict Handling

  Concurrent Writers

  ┌─────────────────────────────────────────────────────────────────┐
  │                 CONCURRENT WRITE CONFLICT                       │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Scenario: Two writers both start from version 1               │
  │                                                                 │
  │   Writer A:                    Writer B:                        │
  │   base_version=1               base_version=1                   │
  │   ┌──────────────┐             ┌──────────────┐                │
  │   │ Start update │             │ Start update │                │
  │   │ Chunk 0 → X  │             │ Chunk 1 → Y  │                │
  │   └──────────────┘             └──────────────┘                │
  │          │                            │                         │
  │          ▼                            ▼                         │
  │   Commit(version=2,            Commit(version=2,                │
  │     base=1)                      base=1)                        │
  │          │                            │                         │
  │          ▼                            ▼                         │
  │   SUCCESS ✓                    CONFLICT ERROR ✗                │
  │   (first to commit wins)       (base version changed)           │
  │                                                                 │
  │   Writer B must:                                                │
  │   1. Fetch current version (now v2)                             │
  │   2. Re-apply changes on top of v2                              │
  │   3. Retry commit as v3                                         │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Optimistic vs Pessimistic Locking

  ┌─────────────────────────────────────────────────────────────────┐
  │                    LOCKING STRATEGIES                           │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   OPTIMISTIC (default):                                         │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ - No locks during edit                                  │  │
  │   │ - Check version at commit time                          │  │
  │   │ - Retry on conflict                                     │  │
  │   │ - Good for: low contention, short edits                 │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   PESSIMISTIC (optional):                                       │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ - Acquire lock before editing                           │  │
  │   │ - Other writers blocked/rejected                        │  │
  │   │ - Lock has lease timeout (prevent deadlock)             │  │
  │   │ - Good for: high contention, long edits                 │  │
  │   │                                                         │  │
  │   │ API: LockFile(path, timeout=300s)                       │  │
  │   │      UnlockFile(path) or auto-release on commit         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  API Examples

  ┌─────────────────────────────────────────────────────────────────┐
  │                      VERSION API                                │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   # Get current version (default)                               │
  │   GET /files/doc.pdf                                            │
  │   → Returns version 3 (current)                                │
  │                                                                 │
  │   # Get specific version                                        │
  │   GET /files/doc.pdf?version=1                                  │
  │   → Returns version 1                                          │
  │                                                                 │
  │   # List all versions                                           │
  │   GET /files/doc.pdf/versions                                   │
  │   → [{version: 1, size: 1024, created_at: ...},                │
  │      {version: 2, size: 1048, created_at: ...},                │
  │      {version: 3, size: 1100, created_at: ...}]                │
  │                                                                 │
  │   # Restore old version                                         │
  │   POST /files/doc.pdf/restore?version=1                         │
  │   → Creates version 4 with same chunks as version 1            │
  │                                                                 │
  │   # Update with conflict detection                              │
  │   PUT /files/doc.pdf                                            │
  │   Headers: If-Match: version=3                                  │
  │   → 200 OK (if current is v3)                                  │
  │   → 409 Conflict (if current is not v3)                        │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Summary

  | Question             | Answer                                                                  |
  |----------------------|-------------------------------------------------------------------------|
  | Why immutable?       | Simplifies consistency, enables safe concurrent access, trivial caching |
  | How track versions?  | Version table links to chunk list; chunks reference counted             |
  | Read during update?  | Reader pins version at start; downloads from immutable chunks           |
  | Conflict handling?   | Optimistic locking with version check; retry on conflict                |
  | Storage efficiency?  | Unchanged chunks reused across versions (deduplication)                 |
  | Old version cleanup? | Policy-based retention; GC chunks when refcount=0                       |

  Would you like to discuss how to implement efficient diff/patch updates for large files, or explore how this versioning system enables features like snapshots and branching?

> How will you split the data (partitioning) and create indexes? 

⏺ Data Partitioning & Indexing Strategy

  Why Partitioning?

  ┌─────────────────────────────────────────────────────────────────┐
  │                    SCALING CHALLENGES                           │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Without partitioning:                                         │
  │                                                                 │
  │   ┌─────────────────────────────────────────┐                  │
  │   │           Single Metadata Node           │                  │
  │   │                                          │                  │
  │   │   100 billion files                      │                  │
  │   │   1 PB metadata                          │                  │
  │   │   100K requests/second                   │                  │
  │   │                                          │                  │
  │   │   ❌ Won't fit in memory                │                  │
  │   │   ❌ Single point of failure            │                  │
  │   │   ❌ CPU bottleneck                     │                  │
  │   └─────────────────────────────────────────┘                  │
  │                                                                 │
  │   With partitioning:                                            │
  │                                                                 │
  │   ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐         │
  │   │ Shard 1  │ │ Shard 2  │ │ Shard 3  │ │ Shard 4  │  ...    │
  │   │ 25B files│ │ 25B files│ │ 25B files│ │ 25B files│         │
  │   │ 25K rps  │ │ 25K rps  │ │ 25K rps  │ │ 25K rps  │         │
  │   └──────────┘ └──────────┘ └──────────┘ └──────────┘         │
  │                                                                 │
  │   ✓ Horizontal scaling                                         │
  │   ✓ Parallel processing                                        │
  │   ✓ Isolated failures                                          │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Two Layers of Partitioning

  ┌─────────────────────────────────────────────────────────────────┐
  │                                                                 │
  │   Layer 1: METADATA PARTITIONING                                │
  │   ─────────────────────────────                                 │
  │   How to split the namespace (directories, files, chunks)       │
  │                                                                 │
  │   Layer 2: CHUNK DATA PARTITIONING                              │
  │   ────────────────────────────────                              │
  │   How to distribute actual file data across storage servers     │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Metadata Partitioning Strategies

  Option 1: Hash-Based Partitioning

  ┌─────────────────────────────────────────────────────────────────┐
  │                    HASH PARTITIONING                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   shard_id = hash(inode_id) % num_shards                        │
  │                                                                 │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │  File: /data/projects/report.pdf                       │   │
  │   │  inode_id: 12345                                       │   │
  │   │  hash(12345) % 4 = 1                                   │   │
  │   │  → Routes to Shard 1                                   │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │   Pros:                                                         │
  │   ✓ Even distribution                                          │
  │   ✓ Simple routing logic                                       │
  │                                                                 │
  │   Cons:                                                         │
  │   ❌ Parent and child may be on different shards               │
  │   ❌ LIST /data/projects requires querying ALL shards          │
  │   ❌ Directory operations become expensive                     │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Option 2: Path-Based (Subtree) Partitioning ✓ Recommended

  ┌─────────────────────────────────────────────────────────────────┐
  │                 SUBTREE PARTITIONING                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Partition by top-level directory or path prefix               │
  │                                                                 │
  │   /                                                             │
  │   ├── /users/       ────────► Shard 1                          │
  │   │   ├── /users/alice/                                        │
  │   │   ├── /users/bob/                                          │
  │   │   └── ...                                                   │
  │   ├── /projects/    ────────► Shard 2                          │
  │   │   ├── /projects/webapp/                                    │
  │   │   └── ...                                                   │
  │   ├── /data/        ────────► Shard 3                          │
  │   │   ├── /data/logs/                                          │
  │   │   └── ...                                                   │
  │   └── /backups/     ────────► Shard 4                          │
  │                                                                 │
  │   Pros:                                                         │
  │   ✓ LIST operations stay within single shard                   │
  │   ✓ Directory tree locality preserved                          │
  │   ✓ Recursive operations efficient                             │
  │                                                                 │
  │   Cons:                                                         │
  │   ❌ Potential hotspots (one directory very popular)           │
  │   ❌ Cross-shard moves require coordination                    │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Option 3: Hybrid (Dynamic Subtree) ✓ Best for Scale

  ┌─────────────────────────────────────────────────────────────────┐
  │                 DYNAMIC SUBTREE PARTITIONING                    │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Start with coarse partitioning, split when hotspot detected   │
  │                                                                 │
  │   Initial:                                                      │
  │   /users/** ──────────────────────► Shard 1                    │
  │                                                                 │
  │   After /users/alice becomes hot:                               │
  │   /users/alice/**    ─────────────► Shard 1a (new)             │
  │   /users/[^alice]/** ─────────────► Shard 1b                   │
  │                                                                 │
  │   Routing Table:                                                │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ prefix              │ shard  │ priority │               │  │
  │   ├─────────────────────────────────────────────────────────┤  │
  │   │ /users/alice/       │ 1a     │ 1        │ (most specific)│ │
  │   │ /users/             │ 1b     │ 2        │               │  │
  │   │ /projects/          │ 2      │ 2        │               │  │
  │   │ /data/              │ 3      │ 2        │               │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   Route by longest prefix match                                 │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Chunk Data Partitioning

  Consistent Hashing for Chunk Placement

  ┌─────────────────────────────────────────────────────────────────┐
  │                   CONSISTENT HASHING                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Hash ring with virtual nodes:                                 │
  │                                                                 │
  │                      0°                                         │
  │                      │                                          │
  │              S1-v1 ──┼── S3-v2                                 │
  │                     ╱│╲                                         │
  │            S2-v2 ──╱ │ ╲── S1-v2                               │
  │                  ╱   │   ╲                                      │
  │        270° ────●────┼────●──── 90°                            │
  │                  ╲   │   ╱                                      │
  │            S3-v1 ──╲ │ ╱── S2-v1                               │
  │                     ╲│╱                                         │
  │              S1-v3 ──┼── S2-v3                                 │
  │                      │                                          │
  │                    180°                                         │
  │                                                                 │
  │   Chunk placement:                                              │
  │   position = hash(chunk_id) % 360                               │
  │   primary = next server clockwise                               │
  │   replicas = next 2 distinct servers clockwise                  │
  │                                                                 │
  │   Example:                                                      │
  │   chunk uuid-A → hash=45° → primary=S1, replicas=[S2, S3]     │
  │                                                                 │
  │   Benefits:                                                     │
  │   ✓ Adding server: only 1/N chunks move                        │
  │   ✓ Even distribution with virtual nodes                       │
  │   ✓ Deterministic placement (no central lookup)                │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Placement with Constraints

  ┌─────────────────────────────────────────────────────────────────┐
  │               RACK/ZONE AWARE PLACEMENT                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Constraints for 3 replicas:                                   │
  │   - Different physical servers                                  │
  │   - Different racks (if possible)                               │
  │   - Different availability zones (if multi-zone)                │
  │                                                                 │
  │   Zone A              Zone B              Zone C                │
  │   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐        │
  │   │ Rack 1      │    │ Rack 3      │    │ Rack 5      │        │
  │   │ ┌─────────┐ │    │ ┌─────────┐ │    │ ┌─────────┐ │        │
  │   │ │Server 1 │ │    │ │Server 5 │ │    │ │Server 9 │ │        │
  │   │ │ C1[P]   │ │    │ │ C1[S]   │ │    │ │ C1[S]   │ │        │
  │   │ └─────────┘ │    │ └─────────┘ │    │ └─────────┘ │        │
  │   │ Rack 2      │    │ Rack 4      │    │ Rack 6      │        │
  │   │ ┌─────────┐ │    │ ┌─────────┐ │    │ ┌─────────┐ │        │
  │   │ │Server 2 │ │    │ │Server 6 │ │    │ │Server 10│ │        │
  │   │ └─────────┘ │    │ └─────────┘ │    │ └─────────┘ │        │
  │   └─────────────┘    └─────────────┘    └─────────────┘        │
  │                                                                 │
  │   Chunk C1 replicas: Server 1 (Zone A), Server 5 (Zone B),     │
  │                      Server 9 (Zone C)                          │
  │   Survives: any single zone failure                             │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Indexing Strategy

  Primary Indexes (Required for Core Operations)

  ┌─────────────────────────────────────────────────────────────────┐
  │                    PRIMARY INDEXES                              │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   INDEX 1: Inode Lookup (by ID)                                 │
  │   ────────────────────────────                                  │
  │   Key:   inode:{inode_id}                                      │
  │   Value: {name, type, size, parent_id, ...}                    │
  │   Use:   Direct file access, metadata retrieval                 │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ inode:12345 → {name:"doc.pdf", type:FILE, size:1048576} │  │
  │   │ inode:12346 → {name:"projects", type:DIR, parent:1}     │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   ─────────────────────────────────────────────────────────────│
  │                                                                 │
  │   INDEX 2: Directory Children (for LIST)                        │
  │   ──────────────────────────────────────                        │
  │   Key:   children:{parent_id}:{child_name}                     │
  │   Value: {child_inode_id}                                      │
  │   Use:   List directory contents, path resolution               │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ children:1:projects     → 12346                         │  │
  │   │ children:1:users        → 12347                         │  │
  │   │ children:12346:webapp   → 12400                         │  │
  │   │ children:12346:doc.pdf  → 12345                         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   PrefixScan("children:12346:") → all children of 12346        │
  │                                                                 │
  │   ─────────────────────────────────────────────────────────────│
  │                                                                 │
  │   INDEX 3: Path to Inode (optional cache)                       │
  │   ───────────────────────────────────────                       │
  │   Key:   path:{full_path}                                      │
  │   Value: {inode_id}                                            │
  │   Use:   Fast path resolution without traversal                 │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ path:/projects/webapp/index.html → 12500                │  │
  │   │ path:/users/alice/config.json    → 12501                │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   Trade-off: Must invalidate on rename/move                     │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Chunk Indexes

  ┌─────────────────────────────────────────────────────────────────┐
  │                     CHUNK INDEXES                               │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   INDEX 4: File → Chunks (for download)                        │
  │   ─────────────────────────────────────                         │
  │   Key:   chunks:{inode_id}:{version}:{chunk_index}             │
  │   Value: {chunk_id, size, checksum}                            │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ chunks:12345:3:0 → {id:uuid-A, size:67108864, ...}      │  │
  │   │ chunks:12345:3:1 → {id:uuid-B, size:67108864, ...}      │  │
  │   │ chunks:12345:3:2 → {id:uuid-C, size:45000000, ...}      │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   ─────────────────────────────────────────────────────────────│
  │                                                                 │
  │   INDEX 5: Chunk → Servers (for routing)                       │
  │   ──────────────────────────────────────                        │
  │   Key:   locations:{chunk_id}                                  │
  │   Value: [{server_id, status}, ...]                            │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ locations:uuid-A → [{server:S1, status:ACTIVE},         │  │
  │   │                     {server:S5, status:ACTIVE},         │  │
  │   │                     {server:S9, status:ACTIVE}]         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   ─────────────────────────────────────────────────────────────│
  │                                                                 │
  │   INDEX 6: Server → Chunks (for rebalancing/GC)                │
  │   ─────────────────────────────────────────────                 │
  │   Key:   server_chunks:{server_id}:{chunk_id}                  │
  │   Value: {status, created_at}                                  │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ server_chunks:S1:uuid-A → {status:ACTIVE, ...}          │  │
  │   │ server_chunks:S1:uuid-D → {status:ACTIVE, ...}          │  │
  │   │ server_chunks:S1:uuid-G → {status:ACTIVE, ...}          │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   PrefixScan("server_chunks:S1:") → all chunks on S1           │
  │   Use: Server decommission, rebalancing                         │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Secondary Indexes (Optional, for Advanced Queries)

  ┌─────────────────────────────────────────────────────────────────┐
  │                   SECONDARY INDEXES                             │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   INDEX 7: By Owner (for quota/billing)                         │
  │   ─────────────────────────────────────                         │
  │   Key:   owner:{user_id}:{inode_id}                            │
  │   Value: {size, created_at}                                    │
  │                                                                 │
  │   Use: "Show all files owned by alice"                          │
  │        "Calculate storage used by alice"                        │
  │                                                                 │
  │   ─────────────────────────────────────────────────────────────│
  │                                                                 │
  │   INDEX 8: By File Type/Extension                               │
  │   ───────────────────────────────                               │
  │   Key:   ext:{extension}:{inode_id}                            │
  │   Value: {path, size}                                          │
  │                                                                 │
  │   Use: "Find all .log files"                                    │
  │        "Find all files larger than 1GB"                         │
  │                                                                 │
  │   ─────────────────────────────────────────────────────────────│
  │                                                                 │
  │   INDEX 9: By Modified Time (for sync/backup)                   │
  │   ───────────────────────────────────────────                   │
  │   Key:   modified:{timestamp}:{inode_id}                       │
  │   Value: {path}                                                │
  │                                                                 │
  │   Use: "Find files modified in last 24 hours"                   │
  │        Incremental backup                                       │
  │                                                                 │
  │   Note: Secondary indexes add write overhead                    │
  │         Only create if query pattern justifies it               │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Path Resolution with Indexes

  ┌─────────────────────────────────────────────────────────────────┐
  │                   PATH RESOLUTION                               │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Resolve: /projects/webapp/src/index.html                      │
  │                                                                 │
  │   Option A: Iterative (using children index)                    │
  │   ──────────────────────────────────────────                    │
  │   1. Start at root (inode_id = 1)                               │
  │   2. Lookup children:1:projects → inode 100                    │
  │   3. Lookup children:100:webapp → inode 200                    │
  │   4. Lookup children:200:src → inode 300                       │
  │   5. Lookup children:300:index.html → inode 400                │
  │   6. Return inode 400                                           │
  │                                                                 │
  │   Cost: O(depth) lookups = 4 lookups                            │
  │                                                                 │
  │   ─────────────────────────────────────────────────────────────│
  │                                                                 │
  │   Option B: Direct (using path index cache)                     │
  │   ─────────────────────────────────────────                     │
  │   1. Lookup path:/projects/webapp/src/index.html → inode 400   │
  │   2. Return inode 400                                           │
  │                                                                 │
  │   Cost: O(1) lookup                                             │
  │                                                                 │
  │   Trade-off:                                                    │
  │   - Path cache must be invalidated on rename/move               │
  │   - Uses more storage                                           │
  │   - Best for read-heavy workloads                               │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Cross-Shard Operations

  Moving Files Across Shards

  ┌─────────────────────────────────────────────────────────────────┐
  │                 CROSS-SHARD MOVE                                │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Move /users/alice/doc.pdf → /projects/webapp/doc.pdf         │
  │                                                                 │
  │   Source: Shard 1 (/users/**)                                   │
  │   Dest:   Shard 2 (/projects/**)                                │
  │                                                                 │
  │   Two-Phase Commit:                                             │
  │                                                                 │
  │   Phase 1: PREPARE                                              │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │ 1. Coordinator → Shard 1: "Prepare to delete inode X"  │   │
  │   │ 2. Coordinator → Shard 2: "Prepare to create inode X'" │   │
  │   │ 3. Both shards: Acquire locks, validate, log intent    │   │
  │   │ 4. Both respond: PREPARED                               │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │   Phase 2: COMMIT                                               │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │ 5. Coordinator → Both: COMMIT                          │   │
  │   │ 6. Shard 1: Remove from /users/alice/                   │   │
  │   │ 7. Shard 2: Add to /projects/webapp/                    │   │
  │   │ 8. Both: Release locks, acknowledge                     │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │   Note: Actual chunk data doesn't move!                         │
  │         Only metadata changes shards                            │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Listing Root Directory (Multi-Shard)

  ┌─────────────────────────────────────────────────────────────────┐
  │               ROOT DIRECTORY LISTING                            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   LIST / (root spans all shards)                                │
  │                                                                 │
  │   Option A: Scatter-Gather                                      │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │                    Coordinator                          │   │
  │   │                         │                               │   │
  │   │          ┌──────────────┼──────────────┐               │   │
  │   │          ▼              ▼              ▼               │   │
  │   │      Shard 1        Shard 2        Shard 3             │   │
  │   │      /users/        /projects/     /data/              │   │
  │   │          │              │              │               │   │
  │   │          └──────────────┼──────────────┘               │   │
  │   │                         ▼                               │   │
  │   │                    Merge results                        │   │
  │   │          [users, projects, data, ...]                   │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │   Option B: Root metadata on dedicated shard                    │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │ Shard 0 (root shard):                                   │   │
  │   │ - Only stores / and top-level directories               │   │
  │   │ - Fast LIST / without scatter-gather                    │   │
  │   │ - Directories route to content shards                   │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  RocksDB Implementation

  Key Encoding for Efficient Scans

  ┌─────────────────────────────────────────────────────────────────┐
  │                 ROCKSDB KEY DESIGN                              │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Key format: {prefix}:{partition_key}:{sort_key}               │
  │                                                                 │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │  Prefix   │ Purpose           │ Example                │   │
  │   ├────────────────────────────────────────────────────────┤   │
  │   │  i:       │ Inode by ID       │ i:12345               │   │
  │   │  c:       │ Children          │ c:12345:filename      │   │
  │   │  k:       │ Chunks            │ k:12345:3:0           │   │
  │   │  l:       │ Chunk locations   │ l:uuid-A              │   │
  │   │  s:       │ Server chunks     │ s:S1:uuid-A           │   │
  │   │  p:       │ Path cache        │ p:/projects/doc.pdf   │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │   Sorted order enables efficient scans:                         │
  │                                                                 │
  │   c:12345:aaa                                                   │
  │   c:12345:bbb      ← PrefixScan("c:12345:") gets all           │
  │   c:12345:ccc                                                   │
  │   c:12346:xxx      ← Different parent, not included            │
  │                                                                 │
  │   Column Families (optional optimization):                      │
  │   ┌────────────────────────────────────────────────────────┐   │
  │   │ CF: metadata  │ Inodes, versions                        │   │
  │   │ CF: directory │ Parent-child relationships              │   │
  │   │ CF: chunks    │ Chunk mappings and locations            │   │
  │   │ CF: indexes   │ Secondary indexes                       │   │
  │   └────────────────────────────────────────────────────────┘   │
  │                                                                 │
  │   Benefits: Separate compaction, bloom filters per CF           │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Summary

  | Aspect                | Strategy                                      |
  |-----------------------|-----------------------------------------------|
  | Metadata partitioning | Dynamic subtree (by path prefix)              |
  | Chunk partitioning    | Consistent hashing with rack awareness        |
  | Primary indexes       | Inode lookup, directory children, file→chunks |
  | Path resolution       | Iterative with optional path cache            |
  | Cross-shard ops       | Two-phase commit for moves                    |
  | Storage engine        | RocksDB with prefix-based keys                |

  Index Summary Table

  | Index         | Key Pattern           | Use Case           |
  |---------------|-----------------------|--------------------|
  | Inode         | i:{inode_id}          | Get file metadata  |
  | Children      | c:{parent}:{name}     | List directory     |
  | Path (cache)  | p:{full_path}         | Fast path lookup   |
  | Chunks        | k:{inode}:{ver}:{idx} | Download file      |
  | Locations     | l:{chunk_id}          | Find chunk servers |
  | Server chunks | s:{server}:{chunk}    | Rebalancing        |

  Would you like me to dive into shard rebalancing strategies, or discuss how to handle hot directories that need to be split dynamically?

> High Availability

How do you handle it if the main control node fails (like the namenode in 
HDFS)?
What is your backup plan (active/standby strategy)?
Do you need tools for coordination like Zookeeper or etcd? 

⏺ High Availability Strategy

  The Single Point of Failure Problem

  ┌─────────────────────────────────────────────────────────────────┐
  │                    HDFS NAMENODE PROBLEM                        │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Traditional HDFS (pre-HA):                                    │
  │                                                                 │
  │                    ┌─────────────┐                              │
  │                    │  NameNode   │                              │
  │                    │  (SINGLE)   │                              │
  │                    └──────┬──────┘                              │
  │                           │                                     │
  │                           ▼                                     │
  │              ┌────────────┴────────────┐                       │
  │              │                         │                       │
  │         DataNode 1              DataNode N                      │
  │                                                                 │
  │   If NameNode dies:                                             │
  │   ❌ Entire cluster unavailable                                │
  │   ❌ Cannot read or write ANY file                             │
  │   ❌ Must wait for manual recovery                             │
  │   ❌ Potential data loss if not properly backed up             │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Our Solution: Raft-Based Consensus

  ┌─────────────────────────────────────────────────────────────────┐
  │                  RAFT CONSENSUS CLUSTER                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   5-Node Metadata Cluster (tolerates 2 failures)                │
  │                                                                 │
  │        ┌─────────┐                                              │
  │        │ Node 1  │◄─────── LEADER                              │
  │        │ (Zone A)│         Handles all writes                   │
  │        └────┬────┘         Can serve reads                      │
  │             │                                                   │
  │      ┌──────┴──────┐                                           │
  │      │   Raft      │                                           │
  │      │ Replication │                                           │
  │      └──────┬──────┘                                           │
  │             │                                                   │
  │   ┌─────────┼─────────┬─────────┬─────────┐                    │
  │   ▼         ▼         ▼         ▼         ▼                    │
  │ ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐                   │
  │ │ N1  │  │ N2  │  │ N3  │  │ N4  │  │ N5  │                   │
  │ │LEADR│  │FOLWR│  │FOLWR│  │FOLWR│  │FOLWR│                   │
  │ │ZoneA│  │ZoneA│  │ZoneB│  │ZoneB│  │ZoneC│                   │
  │ └─────┘  └─────┘  └─────┘  └─────┘  └─────┘                   │
  │                                                                 │
  │   ✓ Automatic leader election                                  │
  │   ✓ No external coordination needed                            │
  │   ✓ Strong consistency guaranteed                              │
  │   ✓ Survives any 2 node failures                               │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Failover Mechanism

  Automatic Leader Election

  ┌─────────────────────────────────────────────────────────────────┐
  │                   LEADER FAILURE SCENARIO                       │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Time ─────────────────────────────────────────────────────►   │
  │                                                                 │
  │   t=0        t=1          t=2          t=3          t=4        │
  │   Normal     Leader       Election     New Leader   Normal      │
  │              Fails        Starts       Elected      Operation   │
  │                                                                 │
  │   ┌─────┐   ┌─────┐      ┌─────┐      ┌─────┐      ┌─────┐    │
  │   │ N1  │   │ N1  │      │ N1  │      │ N1  │      │ N1  │    │
  │   │LEADR│   │ 💀  │      │ 💀  │      │ 💀  │      │ 💀  │    │
  │   └─────┘   └─────┘      └─────┘      └─────┘      └─────┘    │
  │                                                                 │
  │   ┌─────┐   ┌─────┐      ┌─────┐      ┌─────┐      ┌─────┐    │
  │   │ N2  │   │ N2  │      │ N2  │      │ N2  │      │ N2  │    │
  │   │FOLWR│   │FOLWR│      │CANDID│     │LEADR│      │LEADR│    │
  │   └─────┘   └─────┘      └─────┘      └─────┘      └─────┘    │
  │                             │            ▲                      │
  │                             │ Request    │ Majority             │
  │                             │ Votes      │ Granted              │
  │                             ▼            │                      │
  │   ┌─────┐   ┌─────┐      ┌─────┐      ┌─────┐      ┌─────┐    │
  │   │N3,4,5│  │N3,4,5│     │N3,4,5│     │N3,4,5│     │N3,4,5│   │
  │   │FOLWR│   │FOLWR│      │VOTER│      │FOLWR│      │FOLWR│    │
  │   └─────┘   └─────┘      └─────┘      └─────┘      └─────┘    │
  │                                                                 │
  │   Timeline:                                                     │
  │   - Election timeout: 150-300ms (randomized)                    │
  │   - Typical failover: < 1 second                                │
  │   - No manual intervention required                             │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Raft Election Protocol

  ┌─────────────────────────────────────────────────────────────────┐
  │                   RAFT ELECTION DETAILS                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Step 1: Heartbeat Timeout                                     │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ Follower hasn't heard from leader in election_timeout   │  │
  │   │ (randomized 150-300ms to prevent split votes)           │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   Step 2: Become Candidate                                      │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ - Increment current term                                 │  │
  │   │ - Vote for self                                          │  │
  │   │ - Send RequestVote to all other nodes                    │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   Step 3: Collect Votes                                         │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ Other nodes grant vote if:                               │  │
  │   │ - Haven't voted in this term yet                         │  │
  │   │ - Candidate's log is at least as up-to-date              │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   Step 4: Win Election                                          │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ If majority votes received:                              │  │
  │   │ - Become leader                                          │  │
  │   │ - Send heartbeats to establish authority                 │  │
  │   │ - Begin accepting client requests                        │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Active-Standby vs Active-Active

  Option 1: Active-Standby (Traditional)

  ┌─────────────────────────────────────────────────────────────────┐
  │                    ACTIVE-STANDBY                               │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   ┌───────────────────┐         ┌───────────────────┐          │
  │   │    ACTIVE         │         │    STANDBY        │          │
  │   │                   │ ──────► │                   │          │
  │   │  Handles ALL      │  Sync   │  Hot standby      │          │
  │   │  requests         │  logs   │  Ready to take    │          │
  │   │                   │         │  over             │          │
  │   └───────────────────┘         └───────────────────┘          │
  │            ▲                                                    │
  │            │                                                    │
  │      All traffic                                                │
  │                                                                 │
  │   Pros:                                                         │
  │   ✓ Simple to understand                                       │
  │   ✓ No split-brain during normal operation                     │
  │                                                                 │
  │   Cons:                                                         │
  │   ❌ Standby resources wasted                                  │
  │   ❌ Failover delay (detect + switch)                          │
  │   ❌ Risk of split-brain during failover                       │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Option 2: Raft Consensus (Our Choice) ✓

  ┌─────────────────────────────────────────────────────────────────┐
  │                 RAFT CONSENSUS (ACTIVE-ACTIVE READS)            │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │                        ┌─────────┐                              │
  │              ┌────────►│ LEADER  │◄────────┐                   │
  │              │ writes  │  (N1)   │  writes │                   │
  │              │         └────┬────┘         │                   │
  │              │              │              │                   │
  │              │         replication         │                   │
  │              │              │              │                   │
  │        ┌─────┴─────┐   ┌────┴────┐   ┌─────┴─────┐            │
  │        │ FOLLOWER  │   │ FOLLOWER│   │ FOLLOWER  │            │
  │        │   (N2)    │   │  (N3)   │   │   (N4)    │            │
  │        └─────┬─────┘   └────┬────┘   └─────┬─────┘            │
  │              │              │              │                   │
  │              ▼              ▼              ▼                   │
  │           reads          reads          reads                  │
  │        (with ReadIndex)                                        │
  │                                                                 │
  │   Pros:                                                         │
  │   ✓ All nodes actively serving reads                           │
  │   ✓ Automatic failover (built into Raft)                       │
  │   ✓ No external coordinator needed                             │
  │   ✓ Strong consistency guaranteed                              │
  │                                                                 │
  │   Cons:                                                         │
  │   ❌ Writes still go through single leader                     │
  │   ❌ Slightly more complex than active-standby                 │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Do We Need ZooKeeper or etcd?

  Comparison of Approaches

  ┌─────────────────────────────────────────────────────────────────┐
  │              COORDINATION OPTIONS                               │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Option A: External Coordinator (ZooKeeper/etcd)               │
  │   ─────────────────────────────────────────────                 │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │                                                         │  │
  │   │   ┌─────────────┐      ┌─────────────────────────┐     │  │
  │   │   │ ZooKeeper/  │◄────►│   Metadata Service      │     │  │
  │   │   │ etcd cluster│      │   (multiple instances)  │     │  │
  │   │   └─────────────┘      └─────────────────────────┘     │  │
  │   │                                                         │  │
  │   │   ZK/etcd handles:                                      │  │
  │   │   - Leader election                                     │  │
  │   │   - Distributed locks                                   │  │
  │   │   - Configuration                                       │  │
  │   │   - Service discovery                                   │  │
  │   │                                                         │  │
  │   │   Pros: Proven, battle-tested                           │  │
  │   │   Cons: Extra operational burden, another failure point │  │
  │   │                                                         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   ─────────────────────────────────────────────────────────────│
  │                                                                 │
  │   Option B: Embedded Raft (Our Choice) ✓                        │
  │   ──────────────────────────────────────                        │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │                                                         │  │
  │   │   ┌─────────────────────────────────────────────────┐  │  │
  │   │   │         Metadata Service Cluster                │  │  │
  │   │   │                                                 │  │  │
  │   │   │   ┌───────┐   ┌───────┐   ┌───────┐           │  │  │
  │   │   │   │ Node1 │   │ Node2 │   │ Node3 │           │  │  │
  │   │   │   │ ┌───┐ │   │ ┌───┐ │   │ ┌───┐ │           │  │  │
  │   │   │   │ │Raft│◄───┼─►│Raft│◄───┼─►│Raft│           │  │  │
  │   │   │   │ └───┘ │   │ └───┘ │   │ └───┘ │           │  │  │
  │   │   │   └───────┘   └───────┘   └───────┘           │  │  │
  │   │   │                                                 │  │  │
  │   │   └─────────────────────────────────────────────────┘  │  │
  │   │                                                         │  │
  │   │   Raft handles:                                         │  │
  │   │   - Leader election (built-in)                          │  │
  │   │   - Log replication (built-in)                          │  │
  │   │   - Membership changes (built-in)                       │  │
  │   │                                                         │  │
  │   │   Pros: No external dependencies, simpler operations    │  │
  │   │   Cons: Must implement Raft correctly                   │  │
  │   │                                                         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  When to Use External Coordinator

  ┌─────────────────────────────────────────────────────────────────┐
  │            WHEN TO USE ZOOKEEPER/ETCD                           │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   USE external coordinator when:                                │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ ✓ Multiple services need coordination                   │  │
  │   │   (not just metadata service)                           │  │
  │   │                                                         │  │
  │   │ ✓ Already running ZK/etcd for other systems             │  │
  │   │   (Kafka, Kubernetes, etc.)                             │  │
  │   │                                                         │  │
  │   │ ✓ Need complex coordination primitives                  │  │
  │   │   (barriers, queues, watches)                           │  │
  │   │                                                         │  │
  │   │ ✓ Team lacks expertise to implement Raft correctly      │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   USE embedded Raft when:                                       │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │ ✓ Minimizing operational complexity                     │  │
  │   │                                                         │  │
  │   │ ✓ Single system needs consensus                         │  │
  │   │                                                         │  │
  │   │ ✓ Using a database with built-in Raft                   │  │
  │   │   (TiKV, CockroachDB, etcd as library)                  │  │
  │   │                                                         │  │
  │   │ ✓ Want tight integration between consensus and storage  │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Our Recommended Architecture

  ┌─────────────────────────────────────────────────────────────────┐
  │              RECOMMENDED HA ARCHITECTURE                        │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │                      Load Balancer                              │
  │                           │                                     │
  │              ┌────────────┼────────────┐                       │
  │              ▼            ▼            ▼                        │
  │         ┌────────┐   ┌────────┐   ┌────────┐                   │
  │         │ API GW │   │ API GW │   │ API GW │                   │
  │         │  (1)   │   │  (2)   │   │  (3)   │                   │
  │         └───┬────┘   └───┬────┘   └───┬────┘                   │
  │             │            │            │                         │
  │             └────────────┼────────────┘                         │
  │                          │                                      │
  │                          ▼                                      │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │              METADATA SERVICE CLUSTER                   │  │
  │   │                    (Raft-based)                         │  │
  │   │                                                         │  │
  │   │  Zone A          Zone B          Zone C                 │  │
  │   │  ┌─────────┐    ┌─────────┐    ┌─────────┐             │  │
  │   │  │  Node 1 │    │  Node 3 │    │  Node 5 │             │  │
  │   │  │ (LEADER)│    │(FOLLOWER)│   │(FOLLOWER)│            │  │
  │   │  ├─────────┤    ├─────────┤    ├─────────┤             │  │
  │   │  │  Raft   │◄──►│  Raft   │◄──►│  Raft   │             │  │
  │   │  │ RocksDB │    │ RocksDB │    │ RocksDB │             │  │
  │   │  └─────────┘    └─────────┘    └─────────┘             │  │
  │   │  ┌─────────┐    ┌─────────┐                            │  │
  │   │  │  Node 2 │    │  Node 4 │                            │  │
  │   │  │(FOLLOWER)│   │(FOLLOWER)│                           │  │
  │   │  └─────────┘    └─────────┘                            │  │
  │   │                                                         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                          │                                      │
  │                          ▼                                      │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │              CHUNK SERVER CLUSTER                       │  │
  │   │                                                         │  │
  │   │   ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐           │  │
  │   │   │ CS │ │ CS │ │ CS │ │ CS │ │ CS │ │ CS │  ...      │  │
  │   │   │ 1  │ │ 2  │ │ 3  │ │ 4  │ │ 5  │ │ 6  │           │  │
  │   │   └────┘ └────┘ └────┘ └────┘ └────┘ └────┘           │  │
  │   │                                                         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Failure Scenarios & Recovery

  Scenario 1: Leader Node Fails

  ┌─────────────────────────────────────────────────────────────────┐
  │                  LEADER FAILURE RECOVERY                        │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Before:                    After (< 1 second):                │
  │                                                                 │
  │   ┌─────┐                    ┌─────┐                           │
  │   │ N1  │ LEADER ──────────► │ N1  │ DEAD                      │
  │   └─────┘                    └─────┘                           │
  │   ┌─────┐                    ┌─────┐                           │
  │   │ N2  │ FOLLOWER ────────► │ N2  │ NEW LEADER ✓              │
  │   └─────┘                    └─────┘                           │
  │   ┌─────┐                    ┌─────┐                           │
  │   │ N3  │ FOLLOWER ────────► │ N3  │ FOLLOWER                  │
  │   └─────┘                    └─────┘                           │
  │                                                                 │
  │   Impact:                                                       │
  │   - Writes blocked for ~150-500ms during election               │
  │   - Reads continue (from followers with ReadIndex)              │
  │   - No data loss (committed data on majority)                   │
  │   - Automatic, no manual intervention                           │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Scenario 2: Minority Partition

  ┌─────────────────────────────────────────────────────────────────┐
  │                  MINORITY PARTITION                             │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Network partition separates N4, N5 from others:               │
  │                                                                 │
  │   Majority Side (N1, N2, N3)    │    Minority Side (N4, N5)    │
  │   ┌─────┐ ┌─────┐ ┌─────┐      │    ┌─────┐ ┌─────┐          │
  │   │ N1  │ │ N2  │ │ N3  │     ████   │ N4  │ │ N5  │          │
  │   │LEADR│ │FOLWR│ │FOLWR│   PARTITION│FOLWR│ │FOLWR│          │
  │   └─────┘ └─────┘ └─────┘      │    └─────┘ └─────┘          │
  │                                 │                               │
  │   ✓ Continues operating        │    ❌ Cannot serve requests   │
  │   ✓ Reads work                 │    ❌ Cannot elect leader     │
  │   ✓ Writes work                │       (no majority)           │
  │                                 │                               │
  │   When partition heals:                                         │
  │   - N4, N5 rejoin cluster                                       │
  │   - Catch up on missed log entries                              │
  │   - Resume normal operation                                     │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  Scenario 3: Entire Zone Failure

  ┌─────────────────────────────────────────────────────────────────┐
  │                    ZONE FAILURE                                 │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Zone A fails completely (N1, N2 down):                        │
  │                                                                 │
  │   Zone A (DOWN)      Zone B (OK)       Zone C (OK)             │
  │   ┌─────┐ ┌─────┐   ┌─────┐ ┌─────┐   ┌─────┐                 │
  │   │ N1  │ │ N2  │   │ N3  │ │ N4  │   │ N5  │                 │
  │   │ 💀  │ │ 💀  │   │FOLWR│ │FOLWR│   │FOLWR│                 │
  │   └─────┘ └─────┘   └─────┘ └─────┘   └─────┘                 │
  │                          │                │                     │
  │                          ▼                │                     │
  │                     NEW LEADER            │                     │
  │                     (elected from         │                     │
  │                      N3, N4, N5)          │                     │
  │                                                                 │
  │   5 nodes across 3 zones:                                       │
  │   - Zone A: 2 nodes                                             │
  │   - Zone B: 2 nodes                                             │
  │   - Zone C: 1 node                                              │
  │                                                                 │
  │   Can survive:                                                  │
  │   ✓ Any single zone failure (still have 3 nodes = majority)   │
  │   ✓ Any 2 individual node failures                             │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Split-Brain Prevention

  ┌─────────────────────────────────────────────────────────────────┐
  │                  SPLIT-BRAIN PREVENTION                         │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Problem: Two nodes both think they're the leader              │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │               DANGEROUS SPLIT-BRAIN                     │  │
  │   │                                                         │  │
  │   │   ┌─────┐              ┌─────┐                         │  │
  │   │   │ N1  │   NETWORK    │ N2  │                         │  │
  │   │   │"I'm │   PARTITION  │"I'm │                         │  │
  │   │   │leader"│     ████    │leader"│                        │  │
  │   │   └─────┘              └─────┘                         │  │
  │   │      ▲                    ▲                             │  │
  │   │      │                    │                             │  │
  │   │   Clients              Clients                          │  │
  │   │   write X=1            write X=2                        │  │
  │   │                                                         │  │
  │   │   DATA DIVERGENCE! Which X is correct?                  │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   Raft's Solution: MAJORITY REQUIREMENT                         │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │                                                         │  │
  │   │   1. Leader must get majority vote to become leader     │  │
  │   │                                                         │  │
  │   │   2. Writes must replicate to majority before commit    │  │
  │   │                                                         │  │
  │   │   3. Only one node can have majority at any time        │  │
  │   │      (mathematical impossibility to have two)           │  │
  │   │                                                         │  │
  │   │   4. Leader lease: old leader stops serving when        │  │
  │   │      it can't reach majority (lease expires)            │  │
  │   │                                                         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   Result: Split-brain is IMPOSSIBLE with Raft                   │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Chunk Server High Availability

  ┌─────────────────────────────────────────────────────────────────┐
  │              CHUNK SERVER AVAILABILITY                          │
  ├─────────────────────────────────────────────────────────────────┤
  │                                                                 │
  │   Chunk servers are simpler (stateless for coordination):       │
  │                                                                 │
  │   ┌─────────────────────────────────────────────────────────┐  │
  │   │                                                         │  │
  │   │   Chunk C1 with 3 replicas:                             │  │
  │   │                                                         │  │
  │   │   ┌────────┐    ┌────────┐    ┌────────┐              │  │
  │   │   │  CS1   │    │  CS2   │    │  CS3   │              │  │
  │   │   │  C1    │    │  C1'   │    │  C1''  │              │  │
  │   │   │(Zone A)│    │(Zone B)│    │(Zone C)│              │  │
  │   │   └────────┘    └────────┘    └────────┘              │  │
  │   │                                                         │  │
  │   │   If CS1 fails:                                         │  │
  │   │   1. Heartbeat timeout (30 seconds)                     │  │
  │   │   2. Metadata marks CS1 offline                         │  │
  │   │   3. Reads route to CS2 or CS3                          │  │
  │   │   4. Background: re-replicate C1 to CS4                 │  │
  │   │                                                         │  │
  │   └─────────────────────────────────────────────────────────┘  │
  │                                                                 │
  │   No consensus needed for chunk servers:                        │
  │   - Metadata service is source of truth                         │
  │   - Chunk servers just store data                               │
  │   - Replication handles availability                            │
  │                                                                 │
  └─────────────────────────────────────────────────────────────────┘

  ---
  Summary

  | Component        | HA Strategy                              |
  |------------------|------------------------------------------|
  | Metadata Service | 5-node Raft cluster across 3 zones       |
  | Leader Election  | Built into Raft (automatic, <1 sec)      |
  | Coordination     | Embedded Raft (no ZooKeeper needed)      |
  | Split-Brain      | Prevented by majority requirement        |
  | Chunk Servers    | Replication factor 3, automatic failover |

  Key Design Decisions

  | Question          | Answer                                     |
  |-------------------|--------------------------------------------|
  | External ZK/etcd? | No - embedded Raft simpler to operate      |
  | Active-Standby?   | No - Raft with active reads from followers |
  | Failover time?    | < 1 second (automatic)                     |
  | Survives?         | Any 2 node failures, any 1 zone failure    |

  Would you like me to discuss monitoring and alerting strategies for the HA cluster, or dive into how to handle planned maintenance with zero downtime?



